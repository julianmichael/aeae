@article{gantt2021decomposing,
  author    = {William Gantt and
               Lelia Glass and
               Aaron Steven White},
  title     = {Decomposing and Recomposing Event Structure},
  journal   = {CoRR},
  volume    = {abs/2103.10387},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.10387},
  archivePrefix = {arXiv},
  eprint    = {2103.10387},
  timestamp = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-10387.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{coenen2019visualizing,
  title={Visualizing and Measuring the Geometry of BERT},
  author={Andy Coenen and Emily Reif and Ann Yuan and Been Kim and Adam Pearce and F. Vi{\'e}gas and M. Wattenberg},
  booktitle={NeurIPS},
  year={2019}
}

@article{demarneffe2021universal,
  author = {de Marneffe, Marie-Catherine and Manning, Christopher D. and Nivre, Joakim and Zeman, Daniel},
  title = "{Universal Dependencies}",
  journal = {Computational Linguistics},
  volume = {47},
  number = {2},
  pages = {255--308},
  year = {2021},
  month = {07},
  issn = {0891--2017},
  doi = {10.1162/coli_a_00402},
  url = {https://doi.org/10.1162/coli\_a\_00402},
  eprint = {https://direct.mit.edu/coli/article-pdf/47/2/255/1938138/coli\_a\_00402.pdf}
}




@inproceedings{petroni2019language,
    title = {Language Models as Knowledge Bases?},
    author = {Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1250},
    doi = {10.18653/v1/D19-1250},
    pages = {2463--2473},
    abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.}
}

@article{rogers2020primer,
    title = {A Primer in {BERTology}: What we know about how {BERT} works},
    author = {Anna Rogers and Olga Kovaleva and Anna Rumshisky},
    year = {2020},
    journal = {arXiv preprint arXiv:2002.12327}
}

@article{kuznetsov2020matter,
    title = {A Matter of Framing: The Impact of Linguistic Formalism on Probing Results},
    author = {Ilia Kuznetsov and Iryna Gurevych},
    year = {2020},
    journal = {arXiv preprint arXiv:2004.14999}
}

@article{geiger2020modular,
    title = {Modular Representation Underlies Systematic Generalization in Neural Natural Language Inference Models},
    author = {Geiger, Atticus and Richardson, Kyle and Potts, Christopher},
    journal = {arXiv preprint arXiv:2004.14623},
    year = {2020}
}

@inproceedings{wu2020perturbed,
    title = {Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting {BERT}},
    author = {Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    year = {2020},
    doi = {10.18653/v1/2020.acl-main.383},
    pages = {4166--4176},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.acl-main.383},
    month = {July}
}

@inproceedings{chi2020finding,
    title = {Finding Universal Grammatical Relations in Multilingual {BERT}},
    author = {Chi, Ethan A. and Hewitt, John and Manning, Christopher D.},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    year = {2020},
    doi = {10.18653/v1/2020.acl-main.493},
    pages = {5564--5577},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.acl-main.493},
    month = {July}
}

@phdthesis{schuler2005verbnet,
    author = {Schuler, Karin Kipper},
    title = {VerbNet: A broad-coverage, comprehensive verb lexicon},
    school = {University of Pennsylvania},
    year = {2005}
}

@book{levin1993english,
    title = {English Verb Classes and Alternations: A Preliminary Investigation},
    author = {Levin, B.},
    isbn = {9780226475332},
    lccn = {lc92042504},
    url = {https://books.google.com/books?id=6wIZWOrcBf8C},
    year = {1993},
    publisher = {University of Chicago Press}
}

@article{muller2020subclass,
    title = {Subclass Distillation},
    author = {Rafael Müller and Simon Kornblith and Geoffrey Hinton},
    year = {2020},
    journal = {arXiv preprint arXiv:2002.03936}
}

@inproceedings{graves2006connectionist,
    author = {Graves, Alex and Fern\'{a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
    title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
    year = {2006},
    isbn = {1595933832},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1143844.1143891},
    doi = {10.1145/1143844.1143891},
    booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
    pages = {369–376},
    numpages = {8},
    location = {Pittsburgh, Pennsylvania, USA},
    series = {ICML ’06}
}

@inproceedings{kim2019compound,
    title = {Compound Probabilistic Context-Free Grammars for Grammar Induction},
    author = {Kim, Yoon and Dyer, Chris and Rush, Alexander},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1228},
    doi = {10.18653/v1/P19-1228},
    pages = {2369--2385},
    abstract = {We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.}
}

@inproceedings{bowman2016generating,
    title = {Generating Sentences from a Continuous Space},
    author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew and Jozefowicz, Rafal and Bengio, Samy},
    booktitle = {Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning},
    month = {aug},
    year = {2016},
    address = {Berlin, Germany},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/K16-1002},
    doi = {10.18653/v1/K16-1002},
    pages = {10--21}
}

@inproceedings{kingma2014autoencoding,
    title = {Auto-Encoding Variational Bayes},
    author = {Diederik P. Kingma and Max Welling},
    year = {2014},
    booktitle = {International Conference on Learning Representations}
}

@misc{gardner2020evaluating,
    title = {Evaluating {NLP} Models via Contrast Sets},
    author = {Matt Gardner and Yoav Artzi and Victoria Basmova and Jonathan Berant and Ben Bogin and Sihao Chen and Pradeep Dasigi and Dheeru Dua and Yanai Elazar and Ananth Gottumukkala and Nitish Gupta and Hanna Hajishirzi and Gabriel Ilharco and Daniel Khashabi and Kevin Lin and Jiangming Liu and Nelson F. Liu and Phoebe Mulcaire and Qiang Ning and Sameer Singh and Noah A. Smith and Sanjay Subramanian and Reut Tsarfaty and Eric Wallace and Ally Zhang and Ben Zhou},
    year = {2020},
    eprint = {2004.02709},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}

@inproceedings{kaushik2020learning,
    title = {Learning The Difference That Makes A Difference With Counterfactually-Augmented Data},
    author = {Divyansh Kaushik and Eduard Hovy and Zachary Lipton},
    booktitle = {International Conference on Learning Representations},
    year = {2020},
    url = {https://openreview.net/forum?id=Sklgs0NFvr}
}

@inproceedings{qin2019counterfactual,
    title = {Counterfactual Story Reasoning and Generation},
    author = {Qin, Lianhui and Bosselut, Antoine and Holtzman, Ari and Bhagavatula, Chandra and Clark, Elizabeth and Choi, Yejin},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1509},
    doi = {10.18653/v1/D19-1509},
    pages = {5043--5053},
    abstract = {Counterfactual reasoning requires predicting how alternative events, contrary to what actually happened, might have resulted in different outcomes. Despite being considered a necessary component of AI-complete systems, few resources have been developed for evaluating counterfactual reasoning in narratives. In this paper, we propose Counterfactual Story Rewriting: given an original story and an intervening counterfactual event, the task is to minimally revise the story to make it compatible with the given counterfactual event. Solving this task will require deep understanding of causal narrative chains and counterfactual invariance, and integration of such story reasoning capabilities into conditional language generation models. We present TIMETRAVEL, a new dataset of 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. Additionally, we include 81,407 counterfactual {``}branches{''} without a rewritten storyline to support future work on semi- or un-supervised approaches to counterfactual story rewriting. Finally, we evaluate the counterfactual rewriting capacities of several competitive baselines based on pretrained language models, and assess whether common overlap and model-based automatic metrics for text generation correlate well with human scores for counterfactual rewriting.}
}

@article{talmor2019olmpics,
    title = {{oLMpics} -- On what Language Model Pre-training Captures},
    author = {Alon Talmor and Yanai Elazar and Yoav Goldberg and Jonathan Berant},
    year = {2019},
    journal = {arXiv preprint arXiv:1912.13283}
}

@article{voita2020informationtheoretic,
    title = {Information-Theoretic Probing with Minimum Description Length},
    author = {Elena Voita and Ivan Titov},
    year = {2020},
    journal = {arXiv preprint arXiv:2003.12298}
}

@inproceedings{pimentel2020informationtheoretic,
    title = {Information-Theoretic Probing for Linguistic Structure},
    author = {Pimentel, Tiago and Valvoda, Josef and Hall Maudslay, Rowan and Zmigrod, Ran and Williams, Adina and Cotterell, Ryan},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    year = {2020},
    organization = {Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.420},
    pages = {4609--4622},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.acl-main.420},
    month = {July}
}

@inproceedings{petrov2006learning,
    title = {Learning Accurate, Compact, and Interpretable Tree Annotation},
    author = {Petrov, Slav and Barrett, Leon and Thibaux, Romain and Klein, Dan},
    booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
    pages = {433--440},
    year = {2006},
    organization = {Association for Computational Linguistics},
    doi = {10.3115/1220175.1220230},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P06-1055},
    month = {July}
}

@inproceedings{wang2019tell,
    title = {Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling},
    author = {Wang, Alex and Hula, Jan and Xia, Patrick and Pappagari, Raghavendra and McCoy, R. Thomas and Patel, Roma and Kim, Najoung and Tenney, Ian and Huang, Yinghui and Yu, Katherin and Jin, Shuning and Chen, Berlin and Van Durme, Benjamin and Grave, Edouard and Pavlick, Ellie and Bowman, Samuel R.},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1439},
    doi = {10.18653/v1/P19-1439},
    pages = {4465--4476},
    abstract = {Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo{'}s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning BERT on an intermediate task often negatively impacts downstream transfer. In a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research.}
}

@inproceedings{jia2017adversarial,
    title = {Adversarial Examples for Evaluating Reading Comprehension Systems},
    author = {Jia, Robin and Liang, Percy},
    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
    month = {sep},
    year = {2017},
    address = {Copenhagen, Denmark},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D17-1215},
    doi = {10.18653/v1/D17-1215},
    pages = {2021--2031},
    abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75{\%} F1 score to 36{\%}; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7{\%}. We hope our insights will motivate the development of new models that understand language more precisely.}
}

@article{ganin2016domain,
    title = {Domain-adversarial training of neural networks},
    author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
    journal = {The Journal of Machine Learning Research},
    volume = {17},
    number = {1},
    pages = {2096--2030},
    year = {2016},
    publisher = {JMLR. org}
}

@inproceedings{lan2020albert,
    title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
    booktitle = {International Conference on Learning Representations},
    year = {2020},
    url = {https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{liu2019roberta,
    title = {Roberta: A robustly optimized bert pretraining approach},
    author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
    journal = {arXiv preprint arXiv:1907.11692},
    year = {2019}
}

@inproceedings{smilkov2016embedding,
    title = {Embedding projector: Interactive visualization and interpretation of embeddings},
    author = {Smilkov, Daniel and Thorat, Nikhil and Nicholson, Charles and Reif, Emily and Vi{\'e}gas, Fernanda B and Wattenberg, Martin},
    booktitle = {NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems},
    year = {2016}
}

@inproceedings{min2019discrete,
    title = {A Discrete Hard {EM} Approach for Weakly Supervised Question Answering},
    author = {Min, Sewon and Chen, Danqi and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1284},
    doi = {10.18653/v1/D19-1284},
    pages = {2851--2864},
    abstract = {Many question answering (QA) tasks only provide weak supervision for how the answer should be computed. For example, TriviaQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text. In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible solutions (e.g. different mentions or equations) that contains one correct option. We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update. Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2{--}10{\%}, and achieves the state-of-the-art on five of them. Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.}
}

@inproceedings{banko2007open,
    author = {Banko, Michele and Cafarella, Michael J. and Soderland, Stephen and Broadhead, Matt and Etzioni, Oren},
    title = {Open Information Extraction from the Web},
    booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
    series = {IJCAI'07},
    year = {2007},
    location = {Hyderabad, India},
    pages = {2670--2676},
    numpages = {7},
    url = {http://dl.acm.org/citation.cfm?id=1625275.1625705},
    acmid = {1625705},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA}
}

@inproceedings{he2015question,
    title = {Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language},
    author = {He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
    year = {2015},
    doi = {10.18653/v1/D15-1076},
    pages = {643--653},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D15-1076},
    month = {September}
}

@inproceedings{michael2018crowdsourcing,
    title = {Crowdsourcing Question-Answer Meaning Representations},
    author = {Michael, Julian and Stanovsky, Gabriel and He, Luheng and Dagan, Ido and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-2089},
    pages = {560--568},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-2089},
    month = {June}
}

@article{mcinnes2018umap,
    title = {Umap: Uniform manifold approximation and projection for dimension reduction},
    author = {McInnes, Leland and Healy, John and Melville, James},
    journal = {arXiv preprint arXiv:1802.03426},
    year = {2018}
}

@inproceedings{bouma2009normalized,
    title = {Normalized (pointwise) mutual information in collocation extraction},
    author = {Gerlof Bouma},
    booktitle = {GSCL},
    year = {2009}
}

@inproceedings{veldhoen2016diagnostic,
    title = {Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure},
    author = {Sara Veldhoen and Dieuwke Hupkes and Willem H. Zuidema},
    booktitle = {CoCo@NIPS},
    year = {2016}
}

@inproceedings{singh2019bert,
    title = {{BERT} is Not an Interlingua and the Bias of Tokenization},
    author = {Singh, Jasdeep and McCann, Bryan and Socher, Richard and Xiong, Caiming},
    booktitle = {Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-6106},
    doi = {10.18653/v1/D19-6106},
    pages = {47--55},
    abstract = {Multilingual transfer learning can benefit both high- and low-resource languages, but the source of these improvements is not well understood. Cananical Correlation Analysis (CCA) of the internal representations of a pre- trained, multilingual BERT model reveals that the model partitions representations for each language rather than using a common, shared, interlingual space. This effect is magnified at deeper layers, suggesting that the model does not progressively abstract semantic con- tent while disregarding languages. Hierarchical clustering based on the CCA similarity scores between languages reveals a tree structure that mirrors the phylogenetic trees hand- designed by linguists. The subword tokenization employed by BERT provides a stronger bias towards such structure than character- and word-level tokenizations. We release a subset of the XNLI dataset translated into an additional 14 languages at https://www.github.com/salesforce/xnli{\_}extension to assist further research into multilingual representations.}
}

@inproceedings{he2017deep,
    title = {Deep Semantic Role Labeling: What Works and What{'}s Next},
    author = {He, Luheng and Lee, Kenton and Lewis, Mike and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {jul},
    year = {2017},
    address = {Vancouver, Canada},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P17-1044},
    doi = {10.18653/v1/P17-1044},
    pages = {473--483},
    abstract = {We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10{\%} relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.}
}

@inproceedings{kingsbury2002adding,
    author = {Paul Kingsbury and Martha Palmer and Mitch Marcus},
    title = {Adding Semantic Annotation to the Penn TreeBank},
    booktitle = {In Proceedings of the Human Language Technology Conference},
    year = {2002}
}

@article{fillmore2006frame,
    title = {Frame semantics},
    author = {Fillmore, Charles J and others},
    journal = {Cognitive linguistics: Basic readings},
    volume = {34},
    pages = {373--400},
    year = {2006},
    publisher = {Berlin \& New York: Mouton de Gruyter}
}

@article{belinkov2019analysis,
    author = {Belinkov, Yonatan and Glass, James},
    title = {Analysis Methods in Neural Language Processing: A Survey},
    journal = {Transactions of the Association for Computational Linguistics},
    year = {2019},
    volume = {7},
    pages = {49--72},
    doi = {10.1162/tacl_a_00254},
    url = {https://www.aclweb.org/anthology/Q19-1004},
    month = {March}
}

@article{radford2019language,
    title = {Language models are unsupervised multitask learners},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year = {2019},
    journal = {https://blog.openai.com/better-language-models}
}

@inproceedings{liu2019multi,
    title = {Multi-Task Deep Neural Networks for Natural Language Understanding},
    author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1441},
    doi = {10.18653/v1/P19-1441},
    pages = {4487--4496},
    abstract = {In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7{\%} (2.2{\%} absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.}
}

@inproceedings{liu2019linguistic,
    title = {Linguistic Knowledge and Transferability of Contextual Representations},
    author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1112},
    doi = {10.18653/v1/N19-1112},
    pages = {1073--1094},
    abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.}
}

@inproceedings{chen-etal-2019-evaluation,
    title = {Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations},
    author = {Chen, Mingda and Chu, Zewei and Gimpel, Kevin},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1060},
    doi = {10.18653/v1/D19-1060},
    pages = {649--662},
    abstract = {Prior work on pretrained sentence embeddings and benchmarks focus on the capabilities of stand-alone sentences. We propose DiscoEval, a test suite of tasks to evaluate whether sentence representations include broader context information. We also propose a variety of training objectives that makes use of natural annotations from Wikipedia to build sentence encoders capable of modeling discourse. We benchmark sentence encoders pretrained with our proposed training objectives, as well as other popular pretrained sentence encoders on DiscoEval and other sentence evaluation tasks. Empirically, we show that these training objectives help to encode different aspects of information in document structures. Moreover, BERT and ELMo demonstrate strong performances over DiscoEval with individual hidden layers showing different characteristics.}
}

@inproceedings{clark2019what,
    title = {What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention},
    author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
    booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
    year = {2019},
    doi = {10.18653/v1/W19-4828},
    pages = {276--286},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W19-4828},
    month = {August}
}

@incollection{reif2019bertviz,
    title = {Visualizing and Measuring the Geometry of {BERT}},
    author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {8592--8600},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/9065-visualizing-and-measuring-the-geometry-of-bert.pdf}
}

@inproceedings{saphra-lopez-2019-understanding,
    title = {Understanding Learning Dynamics Of Language Models with {SVCCA}},
    author = {Saphra, Naomi and Lopez, Adam},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1329},
    doi = {10.18653/v1/N19-1329},
    pages = {3257--3267},
    abstract = {Research has shown that neural models implicitly encode linguistic features, but there has been no research showing \textit{how} these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis (SVCCA), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations, finding, for example, that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for NLP models, possibly to incorporate linguistic information more effectively.}
}

@inproceedings{abnar2019blackbox,
    title = {Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains},
    author = {Abnar, Samira and Beinborn, Lisa and Choenni, Rochelle and Zuidema, Willem},
    booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
    year = {2019},
    month = {01},
    pages = {191-203},
    doi = {10.18653/v1/W19-4820}
}

@inproceedings{voita2019bottom,
    title = {The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives},
    author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    pages = {4396--4406},
    year = {2019},
    doi = {10.18653/v1/D19-1448},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1448},
    month = {November}
}

@inproceedings{chrupala2019correlating,
    title = {Correlating Neural and Symbolic Representations of Language},
    author = {Chrupa{\l}a, Grzegorz and Alishahi, Afra},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1283},
    doi = {10.18653/v1/P19-1283},
    pages = {2952--2962},
    abstract = {Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then our methods to correlate neural representations of English sentences with their constituency parse trees.}
}

@inproceedings{giulianelli2018under,
    title = {Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information},
    author = {Giulianelli, Mario and Harding, Jack and Mohnert, Florian and Hupkes, Dieuwke and Zuidema, Willem},
    pages = {240--248},
    year = {2018},
    booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
    doi = {10.18653/v1/W18-5426},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-5426},
    month = {November}
}

@inproceedings{bau2018identifying,
    title = {Identifying and Controlling Important Neurons in Neural Machine Translation},
    author = {Anthony Bau and Yonatan Belinkov and Hassan Sajjad and Nadir Durrani and Fahim Dalvi and James Glass},
    booktitle = {Proceedings of ICLR},
    year = {2019},
    url = {https://openreview.net/forum?id=H1z-PsR5KX}
}

@inproceedings{andreas2018measuring,
    title = {Measuring Compositionality in Representation Learning},
    author = {Jacob Andreas},
    booktitle = {Proceedings of ICLR},
    year = {2019},
    url = {https://openreview.net/forum?id=HJz05o0qK7}
}

@inproceedings{singh2018hierarchical,
    title = {Hierarchical interpretations for neural network predictions},
    author = {Chandan Singh and W. James Murdoch and Bin Yu},
    booktitle = {Proceedings of ICLR},
    year = {2019},
    url = {https://openreview.net/forum?id=SkEqro0ctQ}
}

@article{goldberg2019assessing,
    title = {Assessing BERT's Syntactic Abilities},
    author = {Goldberg, Yoav},
    journal = {arXiv preprint arXiv:1901.05287},
    year = {2019}
}

@inproceedings{ghaeini2018interpreting,
    author = {Ghaeini, Reza and Fern, Xiaoli and Tadepalli, Prasad},
    title = {Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    publisher = {Association for Computational Linguistics},
    pages = {4952--4957},
    location = {Brussels, Belgium},
    url = {https://www.aclweb.org/anthology/D18-1537},
    doi = {10.18653/v1/D18-1537},
    month = {October-November}
}

@inproceedings{white2018lexicosyntactic,
    author = {White, Aaron Steven and Rudinger, Rachel and Rawlins, Kyle and Van Durme, Benjamin},
    title = {Lexicosyntactic Inference in Neural Models},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    publisher = {Association for Computational Linguistics},
    pages = {4717--4724},
    location = {Brussels, Belgium},
    url = {https://www.aclweb.org/anthology/D18-1501},
    doi = {10.18653/v1/D18-1501},
    month = {October-November}
}

@inproceedings{godin2018character,
    author = {Godin, Fr{\'e}deric and Demuynck, Kris and Dambre, Joni and De Neve, Wesley and Demeester, Thomas},
    title = {Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    publisher = {Association for Computational Linguistics},
    pages = {3275--3284},
    location = {Brussels, Belgium},
    url = {https://www.aclweb.org/anthology/D18-1365},
    doi = {10.18653/v1/D18-1365},
    month = {October-November}
}

@inproceedings{wu2018phrase,
    author = {Wu, Wei and Wang, Houfeng and Liu, Tianyu and Ma, Shuming},
    title = {Phrase-level Self-Attention Networks for Universal Sentence Encoding},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    publisher = {Association for Computational Linguistics},
    pages = {3729--3738},
    location = {Brussels, Belgium},
    url = {https://www.aclweb.org/anthology/D18-1408},
    doi = {10.18653/v1/D18-1408},
    month = {October-November}
}

@inproceedings{tran2018importance,
    title = {The Importance of Being Recurrent for Modeling Hierarchical Structure},
    author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    pages = {4731--4736},
    year = {2018},
    doi = {10.18653/v1/D18-1503},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1503},
    month = {October-November}
}

@article{futrell2018rnns,
    title = {RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency},
    author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Levy, Roger},
    journal = {arXiv preprint arXiv:1809.01329},
    year = {2018}
}

@inproceedings{marevcek2018extracting,
    title = {Extracting Syntactic Trees from Transformer Encoder Self-Attentions},
    author = {Mare{\v{c}}ek, David and Rosa, Rudolf},
    booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
    pages = {347--349},
    year = {2018},
    doi = {10.18653/v1/W18-5444},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-5444},
    month = {November}
}

@inproceedings{wieting2018no,
    title = {No Training Required: Exploring Random Encoders for Sentence Classification},
    author = {John Wieting and Douwe Kiela},
    booktitle = {Proceedings of ICLR},
    year = {2019},
    url = {https://openreview.net/forum?id=BkgPajAcY7}
}

@inproceedings{manning2014stanford,
    title = {The {S}tanford {C}ore{NLP} Natural Language Processing Toolkit},
    author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
    booktitle = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
    year = {2014},
    doi = {10.3115/v1/P14-5010},
    pages = {55--60},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P14-5010},
    month = {June}
}

@inproceedings{zhang2018,
    title = {Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis},
    author = {Zhang, Kelly and Bowman, Samuel},
    booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
    pages = {359--361},
    year = {2018},
    doi = {10.18653/v1/W18-5448},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-5448},
    month = {November}
}

@inproceedings{Kitaev-Klein:2018:SelfAttentiveParser,
    title = {Constituency Parsing with a Self-Attentive Encoder},
    author = {Kitaev, Nikita and Klein, Dan},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {July},
    year = {2018},
    software = {https://github.com/nikitakit/self-attentive-parser},
    doi = {10.18653/v1/P18-1249},
    pages = {2676--2686},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1249}
}

@article{Baxter:2000:MIB:1622248.1622254,
    author = {Baxter, Jonathan},
    title = {A Model of Inductive Bias Learning},
    journal = {Journal of Artificial Intelligence Research},
    issue_date = {February 2000},
    volume = {12},
    number = {1},
    month = {mar},
    year = {2000},
    issn = {1076-9757},
    pages = {149--198}
}

@inproceedings{gardner2018allennlp,
    title = {{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform},
    author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke},
    booktitle = {Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})},
    pages = {1--6},
    year = {2018},
    doi = {10.18653/v1/W18-2501},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-2501},
    month = {July}
}

@inproceedings{paszke2017automatic,
    title = {Automatic differentiation in {PyTorch}},
    author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
    booktitle = {Proceedings of NIPS},
    year = {2017}
}

@inproceedings{kingma2014adam,
    author = {Diederik P. Kingma and
               Jimmy Ba},
    title = {Adam: {A} Method for Stochastic Optimization},
    booktitle = {Proceedings of ICLR},
    year = {2015}
}

@article{marcus1993building,
    title = {Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank},
    author = {Marcus, Mitchell P. and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
    journal = {Computational Linguistics},
    volume = {19},
    number = {2},
    pages = {313--330},
    year = {1993},
    url = {https://www.aclweb.org/anthology/J93-2004}
}

@misc{nivre2015universal,
    title = {Universal Dependencies 1.2},
    author = {Nivre, Joakim and Agi{\'c}, {\v Z}eljko and Aranzabe, Maria Jesus and Asahara, Masayuki and Atutxa, Aitziber and Ballesteros, Miguel and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bosco, Cristina and Bowman, Sam and Celano, Giuseppe G. A. and Connor, Miriam and de Marneffe, Marie-Catherine and Diaz de Ilarraza, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Erjavec, Toma{\v z} and Farkas, Rich{\'a}rd and Foster, Jennifer and Galbraith, Daniel and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and Goldberg, Yoav and Gonzales, Berta and Guillaume, Bruno and Haji{\v c}, Jan and Haug, Dag and Ion, Radu and Irimia, Elena and Johannsen, Anders and Kanayama, Hiroshi and Kanerva, Jenna and Krek, Simon and Laippala, Veronika and Lenci, Alessandro and Ljube{\v s}i{\'c}, Nikola and Lynn, Teresa and Manning, Christopher and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Ma{\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and Missil{\"a}, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and Mori, Shunsuke and Nurmi, Hanna and Osenova, Petya and {\O}vrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Petrov, Slav and Piitulainen, Jussi and Plank, Barbara and Popel, Martin and Prokopidis, Prokopis and Pyysalo, Sampo and Ramasamy, Loganathan and Rosa, Rudolf and Saleh, Shadi and Schuster, Sebastian and Seeker, Wolfgang and Seraji, Mojgan and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and Simov, Kiril and Smith, Aaron and {\v S}t{\v e}p{\'a}nek, Jan and Suhr, Alane and Sz{\'a}nt{\'o}, Zsolt and Tanaka, Takaaki and Tsarfaty, Reut and Uematsu, Sumire and Uria, Larraitz and Varga, Viktor and Vincze, Veronika and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeman, Daniel and Zhu, Hanzhi},
    url = {http://hdl.handle.net/11234/1-1548},
    note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
    copyright = {Licence Universal Dependencies v1.2},
    year = {2015}
}

@inproceedings{ling2012finegrained,
    author = {Ling, Xiao and Weld, Daniel S.},
    title = {Fine-grained Entity Recognition},
    booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
    series = {AAAI'12},
    year = {2012},
    location = {Toronto, Ontario, Canada},
    pages = {94--100},
    numpages = {7},
    url = {http://dl.acm.org/citation.cfm?id=2900728.2900742},
    acmid = {2900742},
    publisher = {AAAI Press}
}

@inproceedings{church1990word,
    title = {Word Association Norms, Mutual Information, and Lexicography},
    author = {Church, Kenneth Ward and Hanks, Patrick},
    volume = {16},
    number = {1},
    year = {1989},
    url = {https://www.aclweb.org/anthology/P89-1010},
    pages = {76--83},
    booktitle = {27th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.3115/981623.981633},
    publisher = {Association for Computational Linguistics},
    month = {June}
}

@inproceedings{choi2018ultra,
    title = {Ultra-Fine Entity Typing},
    author = {Choi, Eunsol and Levy, Omer and Choi, Yejin and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {jul},
    year = {2018},
    address = {Melbourne, Australia},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1009},
    doi = {10.18653/v1/P18-1009},
    pages = {87--96},
    abstract = {We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks. We present a model that can predict ultra-fine types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking. Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.}
}

@inproceedings{silveira14gold,
    year = {2014},
    author = {Silveira, Natalia and Dozat, Timothy and de Marneffe, Marie-Catherine and Bowman, Samuel and Connor, Miriam and Bauer, John and Manning, Chris},
    title = {A Gold Standard Dependency Corpus for {E}nglish},
    booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
    pages = {2897--2904},
    publisher = {European Language Resources Association (ELRA)},
    url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf},
    month = {May}
}

@article{pradhan2007ontonotes,
    title = {Ontonotes: A unified relational semantic representation},
    author = {Pradhan, Sameer S and Hovy, Eduard and Marcus, Mitch and Palmer, Martha and Ramshaw, Lance and Weischedel, Ralph},
    journal = {International Journal of Semantic Computing},
    volume = {1},
    number = {04},
    pages = {405--419},
    year = {2007}
}

@inproceedings{pradhan2013towards,
    title = {Towards Robust Linguistic Analysis using {O}nto{N}otes},
    author = {Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Ng, Hwee Tou and Bj{\"o}rkelund, Anders and Uryupina, Olga and Zhang, Yuchen and Zhong, Zhi},
    booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
    year = {2013},
    pages = {143--152},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W13-3516},
    month = {August}
}

@article{weischedel2013ontonotes,
    title = {{OntoNotes} Release 5.0 {LDC2013T19}},
    author = {Weischedel, Ralph and Palmer, Martha and Marcus, Mitchell and Hovy, Eduard and Pradhan, Sameer and Ramshaw, Lance and Xue, Nianwen and Taylor, Ann and Kaufman, Jeff and Franchini, Michelle and others},
    journal = {Linguistic Data Consortium, Philadelphia, PA},
    year = {2013}
}

@inproceedings{zhu2015aligning,
    title = {Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {Proceedings of the IEEE international conference on computer vision},
    pages = {19--27},
    year = {2015}
}

@article{palmer2005proposition,
    title = {The {P}roposition {B}ank: An Annotated Corpus of Semantic Roles},
    author = {Palmer, Martha and Gildea, Daniel and Kingsbury, Paul},
    journal = {Computational Linguistics},
    volume = {31},
    number = {1},
    pages = {71--106},
    year = {2005},
    doi = {10.1162/0891201053630264},
    url = {https://www.aclweb.org/anthology/J05-1004}
}

@proceedings{WMT:2017,
    editor = {Bojar, Ond{\v{r}}ej and Buck, Christian and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Koehn, Philipp and Kreutzer, Julia},
    title = {Proceedings of the Second Conference on Machine Translation},
    year = {2017},
    doi = {10.18653/v1/W17-47},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W17-4700},
    month = {September}
}

@inproceedings{chelba2014one,
    title = {One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling},
    author = {Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
    booktitle = {Proceedings of Interspeech},
    year = {2014}
}

@inproceedings{rudinger2018sprl,
    title = {Neural-{D}avidsonian Semantic Proto-role Labeling},
    author = {Rudinger, Rachel and Teichert, Adam and Culkin, Ryan and Zhang, Sheng and Van Durme, Benjamin},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1114},
    pages = {944--955},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1114},
    month = {October-November}
}

@inproceedings{rudinger2018factuality,
    title = {Neural Models of Factuality},
    author = {Rudinger, Rachel and White, Aaron Steven and Van Durme, Benjamin},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-1067},
    pages = {731--744},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-1067},
    month = {June}
}

@inproceedings{white2018lexicosyntactic,
    title = {Lexicosyntactic Inference in Neural Models},
    author = {White, Aaron Steven and Rudinger, Rachel and Rawlins, Kyle and Van Durme, Benjamin},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1501},
    pages = {4717--4724},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1501},
    month = {October-November}
}

@inproceedings{white2017inference,
    title = {Inference is Everything: Recasting Semantic Resources into a Unified Evaluation Framework},
    author = {White, Aaron Steven and Rastogi, Pushpendre and Duh, Kevin and Van Durme, Benjamin},
    booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    year = {2017},
    pages = {996--1005},
    publisher = {Asian Federation of Natural Language Processing},
    url = {https://www.aclweb.org/anthology/I17-1100},
    month = {November}
}

@inproceedings{teichert2017semantic,
    title = {Semantic Proto-Role Labeling},
    author = {Teichert, Adam and Poliak, Adam and {Van Durme}, Benjamin and Gormley, Matthew},
    booktitle = {Proceedings of AAAI},
    year = {2017}
}

@article{reisinger2015semantic,
    author = {Reisinger, Drew and Rudinger, Rachel and Ferraro, Francis and Harman, Craig and Rawlins, Kyle and Van Durme, Benjamin},
    title = {Semantic Proto-Roles},
    journal = {Transactions of the Association for Computational Linguistics},
    year = {2015},
    doi = {10.1162/tacl_a_00152},
    pages = {475--488},
    url = {https://www.aclweb.org/anthology/Q15-1034}
}

@inproceedings{levesque2011winograd,
    author = {Levesque, Hector J. and Davis, Ernest and Morgenstern, Leora},
    title = {The Winograd Schema Challenge},
    booktitle = {Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
    year = {2012}
}

@article{levesque-2014-best,
  author    = {Hector J. Levesque},
  title     = {On our best behaviour},
  journal   = {Artif. Intell.},
  volume    = {212},
  pages     = {27--35},
  year      = {2014},
  url       = {https://doi.org/10.1016/j.artint.2014.03.007},
  doi       = {10.1016/j.artint.2014.03.007},
  timestamp = {Wed, 14 Nov 2018 10:50:00 +0100},
  biburl    = {https://dblp.org/rec/journals/ai/Levesque14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{rahman2012resolving,
    title = {Resolving Complex Cases of Definite Pronouns: The {W}inograd Schema Challenge},
    author = {Rahman, Altaf and Ng, Vincent},
    booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
    year = {2012},
    pages = {777--789},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D12-1071},
    month = {July}
}

@inproceedings{zhang2017tacred,
    author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
    title = {Position-aware Attention and Supervised Data Improve Slot Filling},
    url = {https://www.aclweb.org/anthology/D17-1004},
    pages = {35--45},
    year = {2017},
    doi = {10.18653/v1/D17-1004},
    publisher = {Association for Computational Linguistics},
    month = {September}
}

@inproceedings{devlin2019bert,
    title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1423},
    doi = {10.18653/v1/N19-1423},
    pages = {4171--4186},
    abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@inproceedings{mccann2017learned,
    title = {Learned in translation: Contextualized word vectors},
    author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
    booktitle = {Proceedings of NIPS},
    year = {2017}
}

@inproceedings{peters2018deep,
    title = {Deep Contextualized Word Representations},
    author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-1202},
    pages = {2227--2237},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-1202},
    month = {June}
}

@inproceedings{mikolov2013distributed,
    title = {Distributed representations of words and phrases and their compositionality},
    author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
    booktitle = {Proceedings of NIPS},
    year = {2013}
}

@article{mikolov-etal-2013-efficient,
    title = {Efficient estimation of word representations in vector space},
    author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    journal = {arXiv preprint 1301.3781},
    year = {2013}
}

@inproceedings{pennington2014glove,
    title = {{G}lo{V}e: Global Vectors for Word Representation},
    author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
    booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
    year = {2014},
    doi = {10.3115/v1/D14-1162},
    pages = {1532--1543},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D14-1162},
    month = {October}
}

@article{bojanowski2017fasttext,
    author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
    title = {Enriching Word Vectors with Subword Information},
    journal = {Transactions of the Association for Computational Linguistics},
    year = {2017},
    volume = {5},
    pages = {135--146},
    doi = {10.1162/tacl_a_00051},
    url = {https://www.aclweb.org/anthology/Q17-1010}
}

@article{radford2018improving,
    title = {Improving language understanding by generative pre-training},
    author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
    year = {2018},
    journal = {https://blog.openai.com/language-unsupervised}
}

@article{subramanian2018learning,
    title = {Learning general purpose distributed sentence representations via large scale multi-task learning},
    author = {Subramanian, Sandeep and Trischler, Adam and Bengio, Yoshua and Pal, Christopher J},
    journal = {arXiv preprint 1804.00079},
    year = {2018}
}

@inproceedings{howard2018universal,
    title = {Universal Language Model Fine-tuning for Text Classification},
    author = {Howard, Jeremy and Ruder, Sebastian},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-1031},
    pages = {328--339},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1031},
    month = {July}
}

@article{nie2017dissent,
    title = {Dissent: Sentence representation learning from explicit discourse relations},
    author = {Nie, Allen and Bennett, Erin D and Goodman, Noah D},
    journal = {arXiv preprint arXiv:1710.04334},
    year = {2017}
}

@article{jernite2017discourse,
    title = {Discourse-based objectives for fast unsupervised sentence representation learning},
    author = {Jernite, Yacine and Bowman, Samuel R and Sontag, David},
    journal = {arXiv preprint 1705.00557},
    year = {2017}
}

@article{erhan2010does,
    title = {Why does unsupervised pre-training help deep learning?},
    author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
    journal = {Journal of Machine Learning Research},
    volume = {11},
    number = {Feb},
    pages = {625--660},
    year = {2010}
}

@inproceedings{conneau2017supervised,
    title = {Supervised Learning of Universal Sentence Representations from Natural Language Inference Data},
    author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Lo{\"\i}c and Bordes, Antoine},
    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
    year = {2017},
    doi = {10.18653/v1/D17-1070},
    pages = {670--680},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D17-1070},
    month = {September}
}

@inproceedings{kiros2015skip,
    title = {Skip-thought vectors},
    author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R. and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {Proceedings of NIPS},
    year = {2015}
}

@inproceedings{le2014distributed,
    title = {Distributed representations of sentences and documents},
    author = {Le, Quoc and Mikolov, Tomas},
    booktitle = {Proceedings of ICML},
    year = {2014}
}

@article{cer2018universal,
    title = {Universal sentence encoder},
    author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others},
    journal = {arXiv preprint 1803.11175},
    year = {2018}
}

@inproceedings{conneau2018cram,
    author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"\i}c and Baroni, Marco},
    title = {What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-1198},
    pages = {2126--2136},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1198},
    month = {July}
}

@inproceedings{blevins2018hierarchical,
    author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
    title = {Deep {RNN}s Encode Soft Hierarchical Syntax},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-2003},
    pages = {14--19},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-2003},
    month = {July}
}

@inproceedings{peters2018dissecting,
    title = {Dissecting Contextual Word Embeddings: Architecture and Representation},
    author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1179},
    pages = {1499--1509},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1179},
    month = {October-November}
}

@inproceedings{marvin2018targeted,
    title = {Targeted Syntactic Evaluation of Language Models},
    author = {Marvin, Rebecca and Linzen, Tal},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1151},
    pages = {1192--1202},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1151},
    month = {October-November}
}

@inproceedings{alonso2017multitask,
    title = {When is multitask learning effective? Semantic sequence prediction under varying data conditions},
    author = {Mart{\'\i}nez Alonso, H{\'e}ctor and Plank, Barbara},
    booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
    year = {2017},
    pages = {44--53},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/E17-1005},
    month = {April}
}

@inproceedings{wang2018glue,
    title = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
    booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
    pages = {353--355},
    year = {2018},
    doi = {10.18653/v1/W18-5446},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-5446},
    month = {November}
}

@incollection{wang2019superglue,
    title = {{S}uper{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {3261--3275},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf}
}

@inproceedings{conneau2018senteval,
    author = {Conneau, Alexis and Kiela, Douwe},
    title = {{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations},
    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
    year = {2018},
    publisher = {European Language Resources Association (ELRA)},
    location = {Miyazaki, Japan},
    url = {https://www.aclweb.org/anthology/L18-1269},
    month = {May}
}

@article{dasgupta2018evaluating,
    title = {Evaluating Compositionality in Sentence Embeddings},
    author = {Dasgupta, Ishita and Guo, Demi and Stuhlm{\"u}ller, Andreas and Gershman, Samuel J and Goodman, Noah D},
    journal = {arXiv preprint 1802.04302},
    year = {2018}
}

@inproceedings{adi2016fine,
    title = {Fine-grained analysis of sentence embeddings using auxiliary prediction tasks},
    author = {Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
    booktitle = {International Conference on Learning Representations},
    year = {2017},
    url = {https://openreview.net/forum?id=BJh6Ztuxl}
}

@inproceedings{kuncoro2018lstms,
    title = {{LSTM}s Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better},
    author = {Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-1132},
    pages = {1426--1436},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1132},
    month = {July}
}

@inproceedings{poliak2018collecting,
    title = {Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation},
    author = {Poliak, Adam and Haldar, Aparajita and Rudinger, Rachel and Hu, J. Edward and Pavlick, Ellie and White, Aaron Steven and Van Durme, Benjamin},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1007},
    pages = {67--81},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1007},
    month = {October-November}
}

@article{ahmad2018multi,
    title = {Multi-task Learning for Universal Sentence Representations: What Syntactic and Semantic Information is Captured?},
    author = {Ahmad, Wasi Uddin and Bai, Xueying and Huang, Zhechao and Jiang, Chao and Peng, Nanyun and Chang, Kai-Wei},
    journal = {arXiv preprint 1804.07911},
    year = {2018}
}

@inproceedings{shi2016does,
    title = {Does String-Based Neural {MT} Learn Source Syntax?},
    author = {Shi, Xing and Padhi, Inkit and Knight, Kevin},
    booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    year = {2016},
    doi = {10.18653/v1/D16-1159},
    pages = {1526--1534},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D16-1159},
    month = {November}
}

@inproceedings{belinkov2017neural,
    title = {What do Neural Machine Translation Models Learn about Morphology?},
    author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
    booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2017},
    doi = {10.18653/v1/P17-1080},
    pages = {861--872},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P17-1080},
    month = {July}
}

@inproceedings{belinkov2017evaluating,
    title = {Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks},
    author = {Belinkov, Yonatan and M{\`a}rquez, Llu{\'\i}s and Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Glass, James},
    booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    year = {2017},
    pages = {1--10},
    publisher = {Asian Federation of Natural Language Processing},
    url = {https://www.aclweb.org/anthology/I17-1001},
    month = {November}
}

@phdthesis{belinkov2018thesis,
    title = {On internal language representations in deep learning: An analysis of machine translation and speech recognition},
    author = {Belinkov, Yonatan},
    year = {2018},
    school = {Massachusetts Institute of Technology}
}

@article{linzen2016assessing,
    title = {Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies},
    author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {4},
    number = {1},
    pages = {521--535},
    year = {2016},
    doi = {10.1162/tacl_a_00115},
    url = {https://www.aclweb.org/anthology/Q16-1037}
}

@inproceedings{gulordava2018colorless,
    title = {Colorless Green Recurrent Networks Dream Hierarchically},
    author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-1108},
    pages = {1195--1205},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-1108},
    month = {June}
}

@inproceedings{tran2018importance,
    title = {The Importance of Being Recurrent for Modeling Hierarchical Structure},
    author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
    year = {2018},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/D18-1503},
    pages = {4731--4736},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1503},
    month = {October-November}
}

@inproceedings{ettinger2018assessing,
    author = {Ettinger, Allyson and Elgohary, Ahmed and Phillips, Colin and Resnik, Philip},
    title = {Assessing Composition in Sentence Vector Representations},
    booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
    year = {2018},
    pages = {1790--1801},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/C18-1152},
    month = {August}
}

@inproceedings{gururangan2018annotation,
    title = {Annotation Artifacts in Natural Language Inference Data},
    author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    volume = {2},
    pages = {107--112},
    year = {2018},
    doi = {10.18653/v1/N18-2017},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-2017},
    month = {June}
}

@inproceedings{poliak2018hypothesis,
    title = {Hypothesis Only Baselines in Natural Language Inference},
    author = {Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
    booktitle = {Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics},
    year = {2018},
    doi = {10.18653/v1/S18-2023},
    pages = {180--191},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/S18-2023},
    month = {June}
}

@article{punyakanok2008importance,
    title = {The Importance of Syntactic Parsing and Inference in Semantic Role Labeling},
    author = {Punyakanok, Vasin and Roth, Dan and Yih, Wen-tau},
    journal = {Computational Linguistics},
    volume = {34},
    number = {2},
    pages = {257--287},
    year = {2008},
    doi = {10.1162/coli.2008.34.2.257},
    url = {https://www.aclweb.org/anthology/J08-2005}
}

@inproceedings{gildea2002necessity,
    title = {The Necessity of Parsing for Predicate Argument Recognition},
    author = {Gildea, Daniel and Palmer, Martha},
    booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
    year = {2002},
    doi = {10.3115/1073083.1073124},
    pages = {239--246},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P02-1031},
    month = {July}
}

@inproceedings{hewitt-manning-2019-structural,
    title = {{A} Structural Probe for Finding Syntax in Word Representations},
    author = {Hewitt, John and Manning, Christopher D.},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1419},
    doi = {10.18653/v1/N19-1419},
    pages = {4129--4138},
    abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network{'}s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models{'} vector geometry.}
}

@inproceedings{hewitt-liang-2019-designing,
    title = {Designing and Interpreting Probes with Control Tasks},
    author = {Hewitt, John and Liang, Percy},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1275},
    doi = {10.18653/v1/D19-1275},
    pages = {2733--2743},
    abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe{'}s capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.}
}

@inproceedings{lee2017end,
    title = {End-to-end Neural Coreference Resolution},
    author = {Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
    year = {2017},
    doi = {10.18653/v1/D17-1018},
    pages = {188--197},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D17-1018},
    month = {September}
}

@inproceedings{lee2018higher,
    title = {Higher-Order Coreference Resolution with Coarse-to-Fine Inference},
    author = {Lee, Kenton and He, Luheng and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-2108},
    pages = {687--692},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-2108},
    month = {June}
}

@article{dozat2016deep,
    title = {Deep biaffine attention for neural dependency parsing},
    author = {Dozat, Timothy and Manning, Christopher D},
    journal = {arXiv preprint 1611.01734},
    year = {2016}
}

@inproceedings{he2018jointly,
    author = {He, Luheng and Lee, Kenton and Levy, Omer and Zettlemoyer, Luke},
    title = {Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-2058},
    pages = {364--369},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-2058},
    month = {July}
}

@article{hochreiter1997long,
    title = {Long short-term memory},
    author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
    journal = {Neural computation},
    volume = {9},
    number = {8},
    pages = {1735--1780},
    year = {1997}
}

@inproceedings{vaswani2017attention,
    title = {Attention is all you need},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    booktitle = {Proceedings of NIPS},
    year = {2017}
}

@inproceedings{strubell2018semantics,
    author = {Strubell, Emma and McCallum, Andrew},
    title = {Syntax Helps {ELM}o Understand Semantics: Is Syntax Still Relevant in a Deep Neural Architecture for {SRL}?},
    booktitle = {Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for {NLP}},
    year = {2018},
    doi = {10.18653/v1/W18-2904},
    pages = {19--27},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-2904},
    month = {July}
}

@inproceedings{strubell2018linguistically,
    title = {Linguistically-Informed Self-Attention for Semantic Role Labeling},
    author = {Strubell, Emma and Verga, Patrick and Andor, Daniel and Weiss, David and McCallum, Andrew},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1548},
    pages = {5027--5038},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1548},
    month = {October-November}
}

@inproceedings{sennrich2016neural,
    title = {Neural Machine Translation of Rare Words with Subword Units},
    author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2016},
    doi = {10.18653/v1/P16-1162},
    pages = {1715--1725},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P16-1162},
    month = {August}
}

@article{wu16gnmt,
    title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
    author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
    year = {2016},
    URL = {http://arxiv.org/abs/1609.08144},
    journal = {CoRR},
    volume = {abs/1609.08144}
}

@inproceedings{sutskever2014sequence,
    title = {Sequence to sequence learning with neural networks},
    author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
    booktitle = {Proceedings of NIPS},
    year = {2014}
}

@inproceedings{evaluating-fine-grained-semantic-phenomena-in-neural-machine-translation-encoders-using-entailment,
    author = {Poliak, Adam and Belinkov, Yonatan and Glass, James and Van Durme, Benjamin},
    title = {On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference},
    year = {2018},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    doi = {10.18653/v1/N18-2082},
    pages = {513--523},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-2082},
    month = {June}
}

@proceedings{SemEval:2010,
    editor = {Erk, Katrin and Strapparava, Carlo},
    title = {Proceedings of the 5th International Workshop on Semantic Evaluation},
    month = {July},
    year = {2010},
    address = {Uppsala, Sweden},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/S10-1000}
}

@inproceedings{hendrickx2009semeval,
    title = {{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals},
    author = {Hendrickx, Iris and Kim, Su Nam and Kozareva, Zornitsa and Nakov, Preslav and {\'O} S{\'e}aghdha, Diarmuid and Pad{\'o}, Sebastian and Pennacchiotti, Marco and Romano, Lorenza and Szpakowicz, Stan},
    booktitle = {Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009)},
    year = {2009},
    pages = {94--99},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W09-2415},
    month = {June}
}

@article{amigo2009comparison,
    title = {A comparison of extrinsic clustering evaluation metrics based on formal constraints},
    author = {Amig{\'o}, Enrique and Gonzalo, Julio and Artiles, Javier and Verdejo, Felisa},
    journal = {Information retrieval},
    volume = {12},
    number = {4},
    pages = {461--486},
    year = {2009},
    publisher = {Springer}
}

@inproceedings{bagga-baldwin-1998-bcubed,
    title = "Entity-Based Cross-Document Coreferencing Using the Vector Space Model",
    author = "Bagga, Amit  and
      Baldwin, Breck",
    booktitle = "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",
    month = aug,
    year = "1998",
    address = "Montreal, Quebec, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P98-1012",
    doi = "10.3115/980845.980859",
    pages = "79--85",
}


@inproceedings{bastings-etal-2019-interpretable,
    title = {Interpretable Neural Predictions with Differentiable Binary Variables},
    author = {Bastings, Jasmijn and Aziz, Wilker and Titov, Ivan},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1284},
    doi = {10.18653/v1/P19-1284},
    pages = {2963--2977},
    abstract = {The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification{--}a rationale{--}for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.}
}


@inproceedings{shi-etal-2021-learning,
    title = "Learning Syntax from Naturally-Occurring Bracketings",
    author = "Shi, Tianze  and
      {\.I}rsoy, Ozan  and
      Malioutov, Igor  and
      Lee, Lillian",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.234",
    doi = "10.18653/v1/2021.naacl-main.234",
    pages = "2941--2949"
}

@incollection{fillmore1968case,
  author       = {Charles J. Fillmore}, 
  title        = {The Case for Case},
  booktitle    = {Universals in Linguistic Theory},
  publisher    = {Holt, Rinehart \& Winston},
  year         = 1968,
  editor       = {Emmon Bach and Robert T. Harms},
}

@phdthesis{gruber1965studies,
  author       = {Jeffrey S. Gruber}, 
  title        = {Studies in Lexical Relations},
  school       = {Massachusetts Institute of Technology},
  year         = 1965,
  month        = 10
}

@inproceedings{slonim1999agglomerative,
  author = {Slonim, Noam and Tishby, Naftali},
  title = {Agglomerative Information Bottleneck},
  year = {1999},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  booktitle = {Proceedings of the 12th International Conference on Neural Information Processing Systems},
  pages = {617--623},
  numpages = {7},
  location = {Denver, CO},
  series = {NIPS'99}
}

@article{lee2016learning,
  title={Learning Recurrent Span Representations for Extractive Question Answering},
  author={Kenton Lee and T. Kwiatkowski and Ankur P. Parikh and Dipanjan Das},
  journal={ArXiv},
  year={2016},
  volume={abs/1611.01436}
}

@inproceedings{gal2016theoretically,
  title={A Theoretically Grounded Application of Dropout in Recurrent Neural Networks},
  author={Yarin Gal and Zoubin Ghahramani},
  booktitle={NIPS},
  year={2016}
}

@inproceedings{srivastava2015training,
  title={Training Very Deep Networks},
  author={R. Srivastava and Klaus Greff and J. Schmidhuber},
  booktitle={NIPS},
  year={2015}
}

@book{bresnan-etal-2015-lexical,
  title={Lexical-functional syntax},
  author={Bresnan, Joan and Asudeh, Ash and Toivonen, Ida and Wechsler, Stephen},
  year={2015},
  publisher={John Wiley \& Sons}
}

@book{steedman2000syntactic,
  title={The Syntactic Process},
  author={Steedman, Mark},
  year={2000},
  publisher={MIT Press}
}

@book{steedman1996surface,
  title={Surface Structure and Interpretation},
  author={Steedman, Mark},
  year={1996},
  publisher={MIT Press}
}

@book{pollard1994head,
  title={Head-driven phrase structure grammar},
  author={Pollard, Carl and Sag, Ivan A},
  year={1994},
  publisher={University of Chicago Press}
}

@book{levin-rappaport-2005-argument,
  title = {Argument Realization},
  author = {Levin, Beth and Hovav, Malka Rappaport},
  year = {2005},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK}
}

@inproceedings{christensen2011analysis,
  title={An analysis of open information extraction based on semantic role labeling},
  author={Janara Christensen and Mausam and Stephen Soderland and Oren Etzioni},
  booktitle={K-CAP},
  year={2011}
}

@unpublished{babko-malaya-2005-propbank,
    author = {Olga Babko-Malaya},
    title = {Propbank annotation guidelines},
    year = {2005},
    url = {https://verbs.colorado.edu/~mpalmer/projects/ace/PBguidelines.pdf}
}

@article{lin-1991-divergence,
    author = {Jianhua Lin},
    title = {Divergence measures based on the Shannon entropy},
    journal = {IEEE Transactions on Information theory},
    year = {1991},
    volume = {37},
    pages = {145--151}
}

@article{nigam-etal-2000-text,
  title={Text Classification from Labeled and Unlabeled Documents using EM},
  author={K. Nigam and A. McCallum and S. Thrun and Tom Michael Mitchell},
  journal={Machine Learning},
  year={2000},
  volume={39},
  pages={103-134}
}

@inproceedings{petroni2019language,
    title = {Language Models as Knowledge Bases?},
    author = {Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1250},
    doi = {10.18653/v1/D19-1250},
    pages = {2463--2473},
    abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.}
}

@article{geiger2020modular,
    title = {Modular Representation Underlies Systematic Generalization in Neural Natural Language Inference Models},
    author = {Geiger, Atticus and Richardson, Kyle and Potts, Christopher},
    journal = {arXiv preprint arXiv:2004.14623},
    year = {2020}
}

@inproceedings{wu2020perturbed,
    title = {Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting {BERT}},
    author = {Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    year = {2020},
    doi = {10.18653/v1/2020.acl-main.383},
    pages = {4166--4176},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.acl-main.383},
    month = {July}
}

@inproceedings{chi2020finding,
    title = {Finding Universal Grammatical Relations in Multilingual {BERT}},
    author = {Chi, Ethan A. and Hewitt, John and Manning, Christopher D.},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    year = {2020},
    doi = {10.18653/v1/2020.acl-main.493},
    pages = {5564--5577},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.acl-main.493},
    month = {July}
}


@phdthesis{schuler2005verbnet,
  author = {Schuler, Karin Kipper and Palmer, Martha S.},
  title = {Verbnet: A Broad-Coverage, Comprehensive Verb Lexicon},
  year = {2005},
  isbn = {054220049X},
  publisher = {University of Pennsylvania},
  address = {USA},
  note = {AAI3179808}
}

@book{levin1993english,
    title = {English Verb Classes and Alternations: A Preliminary Investigation},
    author = {Levin, B.},
    isbn = {9780226475332},
    lccn = {lc92042504},
    url = {https://books.google.com/books?id=6wIZWOrcBf8C},
    year = {1993},
    publisher = {University of Chicago Press}
}

@article{muller2020subclass,
    title = {Subclass Distillation},
    author = {Rafael Müller and Simon Kornblith and Geoffrey Hinton},
    year = {2020},
    journal = {arXiv preprint arXiv:2002.03936}
}

@inproceedings{graves2006connectionist,
    author = {Graves, Alex and Fern\'{a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
    title = {Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks},
    year = {2006},
    isbn = {1595933832},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1143844.1143891},
    doi = {10.1145/1143844.1143891},
    booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
    pages = {369–376},
    numpages = {8},
    location = {Pittsburgh, Pennsylvania, USA},
    series = {ICML ’06}
}

@inproceedings{kim2019compound,
    title = {Compound Probabilistic Context-Free Grammars for Grammar Induction},
    author = {Kim, Yoon and Dyer, Chris and Rush, Alexander},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1228},
    doi = {10.18653/v1/P19-1228},
    pages = {2369--2385},
    abstract = {We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.}
}

@inproceedings{bowman2016generating,
    title = {Generating Sentences from a Continuous Space},
    author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew and Jozefowicz, Rafal and Bengio, Samy},
    booktitle = {Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning},
    month = {aug},
    year = {2016},
    address = {Berlin, Germany},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/K16-1002},
    doi = {10.18653/v1/K16-1002},
    pages = {10--21}
}

@inproceedings{kingma2014autoencoding,
    title = {Auto-Encoding Variational Bayes},
    author = {Diederik P. Kingma and Max Welling},
    year = {2014},
    booktitle = {International Conference on Learning Representations}
}

@misc{gardner2020evaluating,
    title = {Evaluating {NLP} Models via Contrast Sets},
    author = {Matt Gardner and Yoav Artzi and Victoria Basmova and Jonathan Berant and Ben Bogin and Sihao Chen and Pradeep Dasigi and Dheeru Dua and Yanai Elazar and Ananth Gottumukkala and Nitish Gupta and Hanna Hajishirzi and Gabriel Ilharco and Daniel Khashabi and Kevin Lin and Jiangming Liu and Nelson F. Liu and Phoebe Mulcaire and Qiang Ning and Sameer Singh and Noah A. Smith and Sanjay Subramanian and Reut Tsarfaty and Eric Wallace and Ally Zhang and Ben Zhou},
    year = {2020},
    eprint = {2004.02709},
    archivePrefix = {arXiv},
    primaryClass = {cs.CL}
}

@inproceedings{kaushik2020learning,
    title = {Learning The Difference That Makes A Difference With Counterfactually-Augmented Data},
    author = {Divyansh Kaushik and Eduard Hovy and Zachary Lipton},
    booktitle = {International Conference on Learning Representations},
    year = {2020},
    url = {https://openreview.net/forum?id=Sklgs0NFvr}
}

@inproceedings{qin2019counterfactual,
    title = {Counterfactual Story Reasoning and Generation},
    author = {Qin, Lianhui and Bosselut, Antoine and Holtzman, Ari and Bhagavatula, Chandra and Clark, Elizabeth and Choi, Yejin},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1509},
    doi = {10.18653/v1/D19-1509},
    pages = {5043--5053},
    abstract = {Counterfactual reasoning requires predicting how alternative events, contrary to what actually happened, might have resulted in different outcomes. Despite being considered a necessary component of AI-complete systems, few resources have been developed for evaluating counterfactual reasoning in narratives. In this paper, we propose Counterfactual Story Rewriting: given an original story and an intervening counterfactual event, the task is to minimally revise the story to make it compatible with the given counterfactual event. Solving this task will require deep understanding of causal narrative chains and counterfactual invariance, and integration of such story reasoning capabilities into conditional language generation models. We present TIMETRAVEL, a new dataset of 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. Additionally, we include 81,407 counterfactual {``}branches{''} without a rewritten storyline to support future work on semi- or un-supervised approaches to counterfactual story rewriting. Finally, we evaluate the counterfactual rewriting capacities of several competitive baselines based on pretrained language models, and assess whether common overlap and model-based automatic metrics for text generation correlate well with human scores for counterfactual rewriting.}
}

@article{talmor2019olmpics,
    title = {{oLMpics} -- On what Language Model Pre-training Captures},
    author = {Alon Talmor and Yanai Elazar and Yoav Goldberg and Jonathan Berant},
    year = {2019},
    journal = {arXiv preprint arXiv:1912.13283}
}

@article{voita2020informationtheoretic,
    title = {Information-Theoretic Probing with Minimum Description Length},
    author = {Elena Voita and Ivan Titov},
    year = {2020},
    journal = {arXiv preprint arXiv:2003.12298}
}

@inproceedings{pimentel2020informationtheoretic,
    title = {Information-Theoretic Probing for Linguistic Structure},
    author = {Pimentel, Tiago and Valvoda, Josef and Hall Maudslay, Rowan and Zmigrod, Ran and Williams, Adina and Cotterell, Ryan},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    year = {2020},
    organization = {Association for Computational Linguistics},
    doi = {10.18653/v1/2020.acl-main.420},
    pages = {4609--4622},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/2020.acl-main.420},
    month = {July}
}

@inproceedings{petrov2006learning,
    title = {Learning Accurate, Compact, and Interpretable Tree Annotation},
    author = {Petrov, Slav and Barrett, Leon and Thibaux, Romain and Klein, Dan},
    booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
    pages = {433--440},
    year = {2006},
    organization = {Association for Computational Linguistics},
    doi = {10.3115/1220175.1220230},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P06-1055},
    month = {July}
}

@inproceedings{wang2019tell,
    title = {Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling},
    author = {Wang, Alex and Hula, Jan and Xia, Patrick and Pappagari, Raghavendra and McCoy, R. Thomas and Patel, Roma and Kim, Najoung and Tenney, Ian and Huang, Yinghui and Yu, Katherin and Jin, Shuning and Chen, Berlin and Van Durme, Benjamin and Grave, Edouard and Pavlick, Ellie and Bowman, Samuel R.},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1439},
    doi = {10.18653/v1/P19-1439},
    pages = {4465--4476},
    abstract = {Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo{'}s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning BERT on an intermediate task often negatively impacts downstream transfer. In a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research.}
}

@inproceedings{jia2017adversarial,
    title = {Adversarial Examples for Evaluating Reading Comprehension Systems},
    author = {Jia, Robin and Liang, Percy},
    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
    month = {sep},
    year = {2017},
    address = {Copenhagen, Denmark},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D17-1215},
    doi = {10.18653/v1/D17-1215},
    pages = {2021--2031},
    abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75{\%} F1 score to 36{\%}; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7{\%}. We hope our insights will motivate the development of new models that understand language more precisely.}
}

@article{ganin2016domain,
    title = {Domain-adversarial training of neural networks},
    author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
    journal = {The Journal of Machine Learning Research},
    volume = {17},
    number = {1},
    pages = {2096--2030},
    year = {2016},
    publisher = {JMLR. org}
}

@inproceedings{lan2020albert,
    title = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
    booktitle = {International Conference on Learning Representations},
    year = {2020},
    url = {https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{liu2019roberta,
    title = {Roberta: A robustly optimized bert pretraining approach},
    author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
    journal = {arXiv preprint arXiv:1907.11692},
    year = {2019}
}

@inproceedings{smilkov2016embedding,
    title = {Embedding projector: Interactive visualization and interpretation of embeddings},
    author = {Smilkov, Daniel and Thorat, Nikhil and Nicholson, Charles and Reif, Emily and Vi{\'e}gas, Fernanda B and Wattenberg, Martin},
    booktitle = {NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems},
    year = {2016}
}

@inproceedings{min2019discrete,
    title = {A Discrete Hard {EM} Approach for Weakly Supervised Question Answering},
    author = {Min, Sewon and Chen, Danqi and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1284},
    doi = {10.18653/v1/D19-1284},
    pages = {2851--2864},
    abstract = {Many question answering (QA) tasks only provide weak supervision for how the answer should be computed. For example, TriviaQA answers are entities that can be mentioned multiple times in supporting documents, while DROP answers can be computed by deriving many different equations from numbers in the reference text. In this paper, we show it is possible to convert such tasks into discrete latent variable learning problems with a precomputed, task-specific set of possible solutions (e.g. different mentions or equations) that contains one correct option. We then develop a hard EM learning scheme that computes gradients relative to the most likely solution at each update. Despite its simplicity, we show that this approach significantly outperforms previous methods on six QA tasks, including absolute gains of 2{--}10{\%}, and achieves the state-of-the-art on five of them. Using hard updates instead of maximizing marginal likelihood is key to these results as it encourages the model to find the one correct answer, which we show through detailed qualitative analysis.}
}

@inproceedings{tenney2019what,
    title = {What do you learn from context? Probing for sentence structure in contextualized word representations},
    author = {Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R Thomas McCoy and Najoung Kim and Benjamin Van Durme and Sam Bowman and Dipanjan Das and Ellie Pavlick},
    booktitle = {International Conference on Learning Representations},
    year = {2019}
}

@inproceedings{tenney2019bert,
    title = {{BERT} Rediscovers the Classical {NLP} Pipeline},
    author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1452},
    doi = {10.18653/v1/P19-1452},
    pages = {4593--4601},
    abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.}
}

@inproceedings{banko2007open,
    author = {Banko, Michele and Cafarella, Michael J. and Soderland, Stephen and Broadhead, Matt and Etzioni, Oren},
    title = {Open Information Extraction from the Web},
    booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
    series = {IJCAI'07},
    year = {2007},
    location = {Hyderabad, India},
    pages = {2670--2676},
    numpages = {7},
    url = {http://dl.acm.org/citation.cfm?id=1625275.1625705},
    acmid = {1625705},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA}
}

@inproceedings{he2015question,
    title = {Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language},
    author = {He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
    year = {2015},
    doi = {10.18653/v1/D15-1076},
    pages = {643--653},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D15-1076},
    month = {September}
}

@inproceedings{michael2018crowdsourcing,
    title = {Crowdsourcing Question-Answer Meaning Representations},
    author = {Michael, Julian and Stanovsky, Gabriel and He, Luheng and Dagan, Ido and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-2089},
    pages = {560--568},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-2089},
    month = {June}
}

@inproceedings{fitzgerald2018large,
    title = {Large-Scale {QA}-{SRL} Parsing},
    author = {FitzGerald, Nicholas and Michael, Julian and He, Luheng and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-1191},
    pages = {2051--2060},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1191},
    month = {July}
}

@article{mcinnes2018umap,
    title = {Umap: Uniform manifold approximation and projection for dimension reduction},
    author = {McInnes, Leland and Healy, John and Melville, James},
    journal = {arXiv preprint arXiv:1802.03426},
    year = {2018}
}

@inproceedings{bouma2009normalized,
    title = {Normalized (pointwise) mutual information in collocation extraction},
    author = {Gerlof Bouma},
    booktitle = {GSCL},
    year = {2009}
}

@inproceedings{veldhoen2016diagnostic,
    title = {Diagnostic Classifiers Revealing how Neural Networks Process Hierarchical Structure},
    author = {Sara Veldhoen and Dieuwke Hupkes and Willem H. Zuidema},
    booktitle = {CoCo@NIPS},
    year = {2016}
}

@inproceedings{khandelwal2018sharp,
    title = {Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context},
    author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {jul},
    year = {2018},
    address = {Melbourne, Australia},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1027},
    doi = {10.18653/v1/P18-1027},
    pages = {284--294},
    abstract = {We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.}
}

@inproceedings{singh2019bert,
    title = {{BERT} is Not an Interlingua and the Bias of Tokenization},
    author = {Singh, Jasdeep and McCann, Bryan and Socher, Richard and Xiong, Caiming},
    booktitle = {Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-6106},
    doi = {10.18653/v1/D19-6106},
    pages = {47--55},
    abstract = {Multilingual transfer learning can benefit both high- and low-resource languages, but the source of these improvements is not well understood. Cananical Correlation Analysis (CCA) of the internal representations of a pre- trained, multilingual BERT model reveals that the model partitions representations for each language rather than using a common, shared, interlingual space. This effect is magnified at deeper layers, suggesting that the model does not progressively abstract semantic con- tent while disregarding languages. Hierarchical clustering based on the CCA similarity scores between languages reveals a tree structure that mirrors the phylogenetic trees hand- designed by linguists. The subword tokenization employed by BERT provides a stronger bias towards such structure than character- and word-level tokenizations. We release a subset of the XNLI dataset translated into an additional 14 languages at https://www.github.com/salesforce/xnli{\_}extension to assist further research into multilingual representations.}
}

@inproceedings{he2017deep,
    title = {Deep Semantic Role Labeling: What Works and What{'}s Next},
    author = {He, Luheng and Lee, Kenton and Lewis, Mike and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {jul},
    year = {2017},
    address = {Vancouver, Canada},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P17-1044},
    doi = {10.18653/v1/P17-1044},
    pages = {473--483},
    abstract = {We introduce a new deep learning model for semantic role labeling (SRL) that significantly improves the state of the art, along with detailed analyses to reveal its strengths and limitations. We use a deep highway BiLSTM architecture with constrained decoding, while observing a number of recent best practices for initialization and regularization. Our 8-layer ensemble model achieves 83.2 F1 on theCoNLL 2005 test set and 83.4 F1 on CoNLL 2012, roughly a 10{\%} relative error reduction over the previous state of the art. Extensive empirical analysis of these gains show that (1) deep models excel at recovering long-distance dependencies but can still make surprisingly obvious errors, and (2) that there is still room for syntactic parsers to improve these results.}
}

@inproceedings{kingsbury2002adding,
    author = {Paul Kingsbury and Martha Palmer and Mitch Marcus},
    title = {Adding Semantic Annotation to the Penn TreeBank},
    booktitle = {In Proceedings of the Human Language Technology Conference},
    year = {2002}
}

@article{fillmore2006frame,
    title = {Frame semantics},
    author = {Fillmore, Charles J and others},
    journal = {Cognitive linguistics: Basic readings},
    volume = {34},
    pages = {373--400},
    year = {2006},
    publisher = {Berlin \& New York: Mouton de Gruyter}
}

@inproceedings{baker1998berkeley,
    title = {The {B}erkeley {F}rame{N}et Project},
    author = {Baker, Collin F. and Fillmore, Charles J. and Lowe, John B.},
    booktitle = {{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics},
    pages = {86--90},
    year = {1998},
    organization = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/C98-1013}
}

@article{belinkov2019analysis,
    author = {Belinkov, Yonatan and Glass, James},
    title = {Analysis Methods in Neural Language Processing: A Survey},
    journal = {Transactions of the Association for Computational Linguistics},
    year = {2019},
    volume = {7},
    pages = {49--72},
    doi = {10.1162/tacl_a_00254},
    url = {https://www.aclweb.org/anthology/Q19-1004},
    month = {March}
}

@article{radford2019language,
    title = {Language models are unsupervised multitask learners},
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    year = {2019},
    journal = {https://blog.openai.com/better-language-models}
}

@inproceedings{liu2019multi,
    title = {Multi-Task Deep Neural Networks for Natural Language Understanding},
    author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1441},
    doi = {10.18653/v1/P19-1441},
    pages = {4487--4496},
    abstract = {In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7{\%} (2.2{\%} absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.}
}

@inproceedings{liu2019linguistic,
    title = {Linguistic Knowledge and Transferability of Contextual Representations},
    author = {Liu, Nelson F. and Gardner, Matt and Belinkov, Yonatan and Peters, Matthew E. and Smith, Noah A.},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1112},
    doi = {10.18653/v1/N19-1112},
    pages = {1073--1094},
    abstract = {Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.}
}

@inproceedings{chen-etal-2019-evaluation,
    title = {Evaluation Benchmarks and Learning Criteria for Discourse-Aware Sentence Representations},
    author = {Chen, Mingda and Chu, Zewei and Gimpel, Kevin},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1060},
    doi = {10.18653/v1/D19-1060},
    pages = {649--662},
    abstract = {Prior work on pretrained sentence embeddings and benchmarks focus on the capabilities of stand-alone sentences. We propose DiscoEval, a test suite of tasks to evaluate whether sentence representations include broader context information. We also propose a variety of training objectives that makes use of natural annotations from Wikipedia to build sentence encoders capable of modeling discourse. We benchmark sentence encoders pretrained with our proposed training objectives, as well as other popular pretrained sentence encoders on DiscoEval and other sentence evaluation tasks. Empirically, we show that these training objectives help to encode different aspects of information in document structures. Moreover, BERT and ELMo demonstrate strong performances over DiscoEval with individual hidden layers showing different characteristics.}
}

@inproceedings{clark2019what,
    title = {What Does {BERT} Look at? An Analysis of {BERT}{'}s Attention},
    author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
    booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
    year = {2019},
    doi = {10.18653/v1/W19-4828},
    pages = {276--286},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W19-4828},
    month = {August}
}

@incollection{reif2019bertviz,
    title = {Visualizing and Measuring the Geometry of {BERT}},
    author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {8592--8600},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/9065-visualizing-and-measuring-the-geometry-of-bert.pdf}
}

@inproceedings{saphra-lopez-2019-understanding,
    title = {Understanding Learning Dynamics Of Language Models with {SVCCA}},
    author = {Saphra, Naomi and Lopez, Adam},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1329},
    doi = {10.18653/v1/N19-1329},
    pages = {3257--3267},
    abstract = {Research has shown that neural models implicitly encode linguistic features, but there has been no research showing \textit{how} these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis (SVCCA), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations, finding, for example, that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for NLP models, possibly to incorporate linguistic information more effectively.}
}

@inproceedings{abnar2019blackbox,
    title = {Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains},
    author = {Abnar, Samira and Beinborn, Lisa and Choenni, Rochelle and Zuidema, Willem},
    booktitle = {Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
    year = {2019},
    month = {01},
    pages = {191-203},
    doi = {10.18653/v1/W19-4820}
}

@inproceedings{voita2019bottom,
    title = {The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives},
    author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    pages = {4396--4406},
    year = {2019},
    doi = {10.18653/v1/D19-1448},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1448},
    month = {November}
}

@inproceedings{chrupala2019correlating,
    title = {Correlating Neural and Symbolic Representations of Language},
    author = {Chrupa{\l}a, Grzegorz and Alishahi, Afra},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1283},
    doi = {10.18653/v1/P19-1283},
    pages = {2952--2962},
    abstract = {Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then our methods to correlate neural representations of English sentences with their constituency parse trees.}
}

@inproceedings{giulianelli2018under,
    title = {Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information},
    author = {Giulianelli, Mario and Harding, Jack and Mohnert, Florian and Hupkes, Dieuwke and Zuidema, Willem},
    pages = {240--248},
    year = {2018},
    booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
    doi = {10.18653/v1/W18-5426},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-5426},
    month = {November}
}

@inproceedings{bau2018identifying,
    title = {Identifying and Controlling Important Neurons in Neural Machine Translation},
    author = {Anthony Bau and Yonatan Belinkov and Hassan Sajjad and Nadir Durrani and Fahim Dalvi and James Glass},
    booktitle = {Proceedings of ICLR},
    year = {2019},
    url = {https://openreview.net/forum?id=H1z-PsR5KX}
}

@inproceedings{andreas2018measuring,
    title = {Measuring Compositionality in Representation Learning},
    author = {Jacob Andreas},
    booktitle = {Proceedings of ICLR},
    year = {2019},
    url = {https://openreview.net/forum?id=HJz05o0qK7}
}

@inproceedings{singh2018hierarchical,
    title = {Hierarchical interpretations for neural network predictions},
    author = {Chandan Singh and W. James Murdoch and Bin Yu},
    booktitle = {Proceedings of ICLR},
    year = {2019},
    url = {https://openreview.net/forum?id=SkEqro0ctQ}
}

@article{goldberg2019assessing,
    title = {Assessing BERT's Syntactic Abilities},
    author = {Goldberg, Yoav},
    journal = {arXiv preprint arXiv:1901.05287},
    year = {2019}
}

@inproceedings{ghaeini2018interpreting,
    author = {Ghaeini, Reza and Fern, Xiaoli and Tadepalli, Prasad},
    title = {Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    publisher = {Association for Computational Linguistics},
    pages = {4952--4957},
    location = {Brussels, Belgium},
    url = {https://www.aclweb.org/anthology/D18-1537},
    doi = {10.18653/v1/D18-1537},
    month = {October-November}
}

@inproceedings{white2018lexicosyntactic,
    author = {White, Aaron Steven and Rudinger, Rachel and Rawlins, Kyle and Van Durme, Benjamin},
    title = {Lexicosyntactic Inference in Neural Models},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    publisher = {Association for Computational Linguistics},
    pages = {4717--4724},
    location = {Brussels, Belgium},
    url = {https://www.aclweb.org/anthology/D18-1501},
    doi = {10.18653/v1/D18-1501},
    month = {October-November}
}

@inproceedings{godin2018character,
    author = {Godin, Fr{\'e}deric and Demuynck, Kris and Dambre, Joni and De Neve, Wesley and Demeester, Thomas},
    title = {Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    publisher = {Association for Computational Linguistics},
    pages = {3275--3284},
    location = {Brussels, Belgium},
    url = {https://www.aclweb.org/anthology/D18-1365},
    doi = {10.18653/v1/D18-1365},
    month = {October-November}
}

@inproceedings{wu2018phrase,
    author = {Wu, Wei and Wang, Houfeng and Liu, Tianyu and Ma, Shuming},
    title = {Phrase-level Self-Attention Networks for Universal Sentence Encoding},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    publisher = {Association for Computational Linguistics},
    pages = {3729--3738},
    location = {Brussels, Belgium},
    url = {https://www.aclweb.org/anthology/D18-1408},
    doi = {10.18653/v1/D18-1408},
    month = {October-November}
}

@inproceedings{tran2018importance,
    title = {The Importance of Being Recurrent for Modeling Hierarchical Structure},
    author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    pages = {4731--4736},
    year = {2018},
    doi = {10.18653/v1/D18-1503},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1503},
    month = {October-November}
}

@article{futrell2018rnns,
    title = {RNNs as psycholinguistic subjects: Syntactic state and grammatical dependency},
    author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Levy, Roger},
    journal = {arXiv preprint arXiv:1809.01329},
    year = {2018}
}

@inproceedings{marevcek2018extracting,
    title = {Extracting Syntactic Trees from Transformer Encoder Self-Attentions},
    author = {Mare{\v{c}}ek, David and Rosa, Rudolf},
    booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
    pages = {347--349},
    year = {2018},
    doi = {10.18653/v1/W18-5444},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-5444},
    month = {November}
}

@inproceedings{wieting2018no,
    title = {No Training Required: Exploring Random Encoders for Sentence Classification},
    author = {John Wieting and Douwe Kiela},
    booktitle = {Proceedings of ICLR},
    year = {2019},
    url = {https://openreview.net/forum?id=BkgPajAcY7}
}

@inproceedings{manning2014stanford,
    title = {The {S}tanford {C}ore{NLP} Natural Language Processing Toolkit},
    author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
    booktitle = {Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
    year = {2014},
    doi = {10.3115/v1/P14-5010},
    pages = {55--60},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P14-5010},
    month = {June}
}

@inproceedings{zhang2018,
    title = {Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis},
    author = {Zhang, Kelly and Bowman, Samuel},
    booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
    pages = {359--361},
    year = {2018},
    doi = {10.18653/v1/W18-5448},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-5448},
    month = {November}
}

@inproceedings{Kitaev-Klein:2018:SelfAttentiveParser,
    title = {Constituency Parsing with a Self-Attentive Encoder},
    author = {Kitaev, Nikita and Klein, Dan},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {July},
    year = {2018},
    software = {https://github.com/nikitakit/self-attentive-parser},
    doi = {10.18653/v1/P18-1249},
    pages = {2676--2686},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1249}
}

@article{Baxter:2000:MIB:1622248.1622254,
    author = {Baxter, Jonathan},
    title = {A Model of Inductive Bias Learning},
    journal = {Journal of Artificial Intelligence Research},
    issue_date = {February 2000},
    volume = {12},
    number = {1},
    month = {mar},
    year = {2000},
    issn = {1076-9757},
    pages = {149--198}
}

@inproceedings{gardner2018allennlp,
    title = {{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform},
    author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke},
    booktitle = {Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})},
    pages = {1--6},
    year = {2018},
    doi = {10.18653/v1/W18-2501},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-2501},
    month = {July}
}

@inproceedings{paszke2017automatic,
    title = {Automatic differentiation in {PyTorch}},
    author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
    booktitle = {Proceedings of NIPS},
    year = {2017}
}

@inproceedings{kingma2014adam,
    author = {Diederik P. Kingma and
               Jimmy Ba},
    title = {Adam: {A} Method for Stochastic Optimization},
    booktitle = {Proceedings of ICLR},
    year = {2015}
}

@article{marcus1993building,
    title = {Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank},
    author = {Marcus, Mitchell P. and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
    journal = {Computational Linguistics},
    volume = {19},
    number = {2},
    pages = {313--330},
    year = {1993},
    url = {https://www.aclweb.org/anthology/J93-2004}
}

@misc{nivre2015universal,
    title = {Universal Dependencies 1.2},
    author = {Nivre, Joakim and Agi{\'c}, {\v Z}eljko and Aranzabe, Maria Jesus and Asahara, Masayuki and Atutxa, Aitziber and Ballesteros, Miguel and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bosco, Cristina and Bowman, Sam and Celano, Giuseppe G. A. and Connor, Miriam and de Marneffe, Marie-Catherine and Diaz de Ilarraza, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Erjavec, Toma{\v z} and Farkas, Rich{\'a}rd and Foster, Jennifer and Galbraith, Daniel and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and Goldberg, Yoav and Gonzales, Berta and Guillaume, Bruno and Haji{\v c}, Jan and Haug, Dag and Ion, Radu and Irimia, Elena and Johannsen, Anders and Kanayama, Hiroshi and Kanerva, Jenna and Krek, Simon and Laippala, Veronika and Lenci, Alessandro and Ljube{\v s}i{\'c}, Nikola and Lynn, Teresa and Manning, Christopher and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Mart{\'{\i}}nez Alonso, H{\'e}ctor and Ma{\v s}ek, Jan and Matsumoto, Yuji and {McDonald}, Ryan and Missil{\"a}, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and Mori, Shunsuke and Nurmi, Hanna and Osenova, Petya and {\O}vrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Petrov, Slav and Piitulainen, Jussi and Plank, Barbara and Popel, Martin and Prokopidis, Prokopis and Pyysalo, Sampo and Ramasamy, Loganathan and Rosa, Rudolf and Saleh, Shadi and Schuster, Sebastian and Seeker, Wolfgang and Seraji, Mojgan and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and Simov, Kiril and Smith, Aaron and {\v S}t{\v e}p{\'a}nek, Jan and Suhr, Alane and Sz{\'a}nt{\'o}, Zsolt and Tanaka, Takaaki and Tsarfaty, Reut and Uematsu, Sumire and Uria, Larraitz and Varga, Viktor and Vincze, Veronika and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeman, Daniel and Zhu, Hanzhi},
    url = {http://hdl.handle.net/11234/1-1548},
    note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
    copyright = {Licence Universal Dependencies v1.2},
    year = {2015}
}

@inproceedings{ling2012finegrained,
    author = {Ling, Xiao and Weld, Daniel S.},
    title = {Fine-grained Entity Recognition},
    booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
    series = {AAAI'12},
    year = {2012},
    location = {Toronto, Ontario, Canada},
    pages = {94--100},
    numpages = {7},
    url = {http://dl.acm.org/citation.cfm?id=2900728.2900742},
    acmid = {2900742},
    publisher = {AAAI Press}
}

@inproceedings{church1990word,
    title = {Word Association Norms, Mutual Information, and Lexicography},
    author = {Church, Kenneth Ward and Hanks, Patrick},
    volume = {16},
    number = {1},
    year = {1989},
    url = {https://www.aclweb.org/anthology/P89-1010},
    pages = {76--83},
    booktitle = {27th Annual Meeting of the Association for Computational Linguistics},
    doi = {10.3115/981623.981633},
    publisher = {Association for Computational Linguistics},
    month = {June}
}

@inproceedings{choi2018ultra,
    title = {Ultra-Fine Entity Typing},
    author = {Choi, Eunsol and Levy, Omer and Choi, Yejin and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {jul},
    year = {2018},
    address = {Melbourne, Australia},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1009},
    doi = {10.18653/v1/P18-1009},
    pages = {87--96},
    abstract = {We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks. We present a model that can predict ultra-fine types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking. Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.}
}

@inproceedings{silveira14gold,
    year = {2014},
    author = {Silveira, Natalia and Dozat, Timothy and de Marneffe, Marie-Catherine and Bowman, Samuel and Connor, Miriam and Bauer, John and Manning, Chris},
    title = {A Gold Standard Dependency Corpus for {E}nglish},
    booktitle = {Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)},
    pages = {2897--2904},
    publisher = {European Language Resources Association (ELRA)},
    url = {http://www.lrec-conf.org/proceedings/lrec2014/pdf/1089_Paper.pdf},
    month = {May}
}

@article{pradhan2007ontonotes,
    title = {Ontonotes: A unified relational semantic representation},
    author = {Pradhan, Sameer S and Hovy, Eduard and Marcus, Mitch and Palmer, Martha and Ramshaw, Lance and Weischedel, Ralph},
    journal = {International Journal of Semantic Computing},
    volume = {1},
    number = {04},
    pages = {405--419},
    year = {2007}
}

@inproceedings{pradhan2013towards,
    title = {Towards Robust Linguistic Analysis using {O}nto{N}otes},
    author = {Pradhan, Sameer and Moschitti, Alessandro and Xue, Nianwen and Ng, Hwee Tou and Bj{\"o}rkelund, Anders and Uryupina, Olga and Zhang, Yuchen and Zhong, Zhi},
    booktitle = {Proceedings of the Seventeenth Conference on Computational Natural Language Learning},
    year = {2013},
    pages = {143--152},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W13-3516},
    month = {August}
}

@article{weischedel2013ontonotes,
    title = {{OntoNotes} Release 5.0 {LDC2013T19}},
    author = {Weischedel, Ralph and Palmer, Martha and Marcus, Mitchell and Hovy, Eduard and Pradhan, Sameer and Ramshaw, Lance and Xue, Nianwen and Taylor, Ann and Kaufman, Jeff and Franchini, Michelle and others},
    journal = {Linguistic Data Consortium, Philadelphia, PA},
    year = {2013}
}

@inproceedings{zhu2015aligning,
    title = {Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
    author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {Proceedings of the IEEE international conference on computer vision},
    pages = {19--27},
    year = {2015}
}

@article{palmer2005proposition,
    title = {The {P}roposition {B}ank: An Annotated Corpus of Semantic Roles},
    author = {Palmer, Martha and Gildea, Daniel and Kingsbury, Paul},
    journal = {Computational Linguistics},
    volume = {31},
    number = {1},
    pages = {71--106},
    year = {2005},
    doi = {10.1162/0891201053630264},
    url = {https://www.aclweb.org/anthology/J05-1004}
}

@proceedings{WMT:2017,
    editor = {Bojar, Ond{\v{r}}ej and Buck, Christian and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Koehn, Philipp and Kreutzer, Julia},
    title = {Proceedings of the Second Conference on Machine Translation},
    year = {2017},
    doi = {10.18653/v1/W17-47},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W17-4700},
    month = {September}
}

@inproceedings{chelba2014one,
    title = {One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling},
    author = {Chelba, Ciprian and Mikolov, Tomas and Schuster, Mike and Ge, Qi and Brants, Thorsten and Koehn, Phillipp and Robinson, Tony},
    booktitle = {Proceedings of Interspeech},
    year = {2014}
}

@inproceedings{rudinger2018sprl,
    title = {Neural-{D}avidsonian Semantic Proto-role Labeling},
    author = {Rudinger, Rachel and Teichert, Adam and Culkin, Ryan and Zhang, Sheng and Van Durme, Benjamin},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1114},
    pages = {944--955},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1114},
    month = {October-November}
}

@inproceedings{rudinger2018factuality,
    title = {Neural Models of Factuality},
    author = {Rudinger, Rachel and White, Aaron Steven and Van Durme, Benjamin},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-1067},
    pages = {731--744},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-1067},
    month = {June}
}

@inproceedings{white2018lexicosyntactic,
    title = {Lexicosyntactic Inference in Neural Models},
    author = {White, Aaron Steven and Rudinger, Rachel and Rawlins, Kyle and Van Durme, Benjamin},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1501},
    pages = {4717--4724},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1501},
    month = {October-November}
}

@inproceedings{white2017inference,
    title = {Inference is Everything: Recasting Semantic Resources into a Unified Evaluation Framework},
    author = {White, Aaron Steven and Rastogi, Pushpendre and Duh, Kevin and Van Durme, Benjamin},
    booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    year = {2017},
    pages = {996--1005},
    publisher = {Asian Federation of Natural Language Processing},
    url = {https://www.aclweb.org/anthology/I17-1100},
    month = {November}
}

@inproceedings{teichert2017semantic,
    title = {Semantic Proto-Role Labeling},
    author = {Teichert, Adam and Poliak, Adam and {Van Durme}, Benjamin and Gormley, Matthew},
    booktitle = {Proceedings of AAAI},
    year = {2017}
}

@article{reisinger2015semantic,
    author = {Reisinger, Drew and Rudinger, Rachel and Ferraro, Francis and Harman, Craig and Rawlins, Kyle and Van Durme, Benjamin},
    title = {Semantic Proto-Roles},
    journal = {Transactions of the Association for Computational Linguistics},
    year = {2015},
    doi = {10.1162/tacl_a_00152},
    pages = {475--488},
    url = {https://www.aclweb.org/anthology/Q15-1034}
}

@inproceedings{levesque2011winograd,
    author = {Levesque, Hector J. and Davis, Ernest and Morgenstern, Leora},
    title = {The Winograd Schema Challenge},
    booktitle = {Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning},
    year = {2012}
}

@inproceedings{rahman2012resolving,
    title = {Resolving Complex Cases of Definite Pronouns: The {W}inograd Schema Challenge},
    author = {Rahman, Altaf and Ng, Vincent},
    booktitle = {Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
    year = {2012},
    pages = {777--789},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D12-1071},
    month = {July}
}

@inproceedings{zhang2017tacred,
    author = {Zhang, Yuhao and Zhong, Victor and Chen, Danqi and Angeli, Gabor and Manning, Christopher D.},
    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
    title = {Position-aware Attention and Supervised Data Improve Slot Filling},
    url = {https://www.aclweb.org/anthology/D17-1004},
    pages = {35--45},
    year = {2017},
    doi = {10.18653/v1/D17-1004},
    publisher = {Association for Computational Linguistics},
    month = {September}
}

@inproceedings{devlin2019bert,
    title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1423},
    doi = {10.18653/v1/N19-1423},
    pages = {4171--4186},
    abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@inproceedings{mccann2017learned,
    title = {Learned in translation: Contextualized word vectors},
    author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
    booktitle = {Proceedings of NIPS},
    year = {2017}
}

@inproceedings{peters2018deep,
    title = {Deep Contextualized Word Representations},
    author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-1202},
    pages = {2227--2237},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-1202},
    month = {June}
}

@inproceedings{mikolov2013distributed,
    title = {Distributed representations of words and phrases and their compositionality},
    author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
    booktitle = {Proceedings of NIPS},
    year = {2013}
}

@article{mikolov2013efficient,
    title = {Efficient estimation of word representations in vector space},
    author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
    journal = {arXiv preprint 1301.3781},
    year = {2013}
}

@inproceedings{pennington2014glove,
    title = {{G}lo{V}e: Global Vectors for Word Representation},
    author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
    booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
    year = {2014},
    doi = {10.3115/v1/D14-1162},
    pages = {1532--1543},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D14-1162},
    month = {October}
}

@article{bojanowski2017fasttext,
    author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
    title = {Enriching Word Vectors with Subword Information},
    journal = {Transactions of the Association for Computational Linguistics},
    year = {2017},
    volume = {5},
    pages = {135--146},
    doi = {10.1162/tacl_a_00051},
    url = {https://www.aclweb.org/anthology/Q17-1010}
}

@article{radford2018improving,
    title = {Improving language understanding by generative pre-training},
    author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
    year = {2018},
    journal = {https://blog.openai.com/language-unsupervised}
}

@article{subramanian2018learning,
    title = {Learning general purpose distributed sentence representations via large scale multi-task learning},
    author = {Subramanian, Sandeep and Trischler, Adam and Bengio, Yoshua and Pal, Christopher J},
    journal = {arXiv preprint 1804.00079},
    year = {2018}
}

@inproceedings{howard2018universal,
    title = {Universal Language Model Fine-tuning for Text Classification},
    author = {Howard, Jeremy and Ruder, Sebastian},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-1031},
    pages = {328--339},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1031},
    month = {July}
}

@article{nie2017dissent,
    title = {Dissent: Sentence representation learning from explicit discourse relations},
    author = {Nie, Allen and Bennett, Erin D and Goodman, Noah D},
    journal = {arXiv preprint arXiv:1710.04334},
    year = {2017}
}

@article{jernite2017discourse,
    title = {Discourse-based objectives for fast unsupervised sentence representation learning},
    author = {Jernite, Yacine and Bowman, Samuel R and Sontag, David},
    journal = {arXiv preprint 1705.00557},
    year = {2017}
}

@article{erhan2010does,
    title = {Why does unsupervised pre-training help deep learning?},
    author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
    journal = {Journal of Machine Learning Research},
    volume = {11},
    number = {Feb},
    pages = {625--660},
    year = {2010}
}

@inproceedings{conneau2017supervised,
    title = {Supervised Learning of Universal Sentence Representations from Natural Language Inference Data},
    author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Lo{\"\i}c and Bordes, Antoine},
    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
    year = {2017},
    doi = {10.18653/v1/D17-1070},
    pages = {670--680},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D17-1070},
    month = {September}
}

@inproceedings{kiros2015skip,
    title = {Skip-thought vectors},
    author = {Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan R. and Zemel, Richard and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    booktitle = {Proceedings of NIPS},
    year = {2015}
}

@inproceedings{le2014distributed,
    title = {Distributed representations of sentences and documents},
    author = {Le, Quoc and Mikolov, Tomas},
    booktitle = {Proceedings of ICML},
    year = {2014}
}

@article{cer2018universal,
    title = {Universal sentence encoder},
    author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others},
    journal = {arXiv preprint 1803.11175},
    year = {2018}
}

@inproceedings{conneau2018cram,
    author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"\i}c and Baroni, Marco},
    title = {What you can cram into a single {\$}{\&}!{\#}* vector: Probing sentence embeddings for linguistic properties},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-1198},
    pages = {2126--2136},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1198},
    month = {July}
}

@inproceedings{blevins2018hierarchical,
    author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
    title = {Deep {RNN}s Encode Soft Hierarchical Syntax},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-2003},
    pages = {14--19},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-2003},
    month = {July}
}

@inproceedings{peters2018dissecting,
    title = {Dissecting Contextual Word Embeddings: Architecture and Representation},
    author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1179},
    pages = {1499--1509},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1179},
    month = {October-November}
}

@inproceedings{marvin2018targeted,
    title = {Targeted Syntactic Evaluation of Language Models},
    author = {Marvin, Rebecca and Linzen, Tal},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1151},
    pages = {1192--1202},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1151},
    month = {October-November}
}

@inproceedings{alonso2017multitask,
    title = {When is multitask learning effective? Semantic sequence prediction under varying data conditions},
    author = {Mart{\'\i}nez Alonso, H{\'e}ctor and Plank, Barbara},
    booktitle = {Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
    year = {2017},
    pages = {44--53},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/E17-1005},
    month = {April}
}

@inproceedings{wang-etal-2019-glue,
  author    = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=rJ4km2R5t7},
  timestamp = {Thu, 25 Jul 2019 14:25:46 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/WangSMHLB19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{wang-etal-2019-superglue,
    title = {{S}uper{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {3261--3275},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/8589-superglue-a-stickier-benchmark-for-general-purpose-language-understanding-systems.pdf}
}

@inproceedings{conneau2018senteval,
    author = {Conneau, Alexis and Kiela, Douwe},
    title = {{S}ent{E}val: An Evaluation Toolkit for Universal Sentence Representations},
    booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
    year = {2018},
    publisher = {European Language Resources Association (ELRA)},
    location = {Miyazaki, Japan},
    url = {https://www.aclweb.org/anthology/L18-1269},
    month = {May}
}

@article{dasgupta2018evaluating,
    title = {Evaluating Compositionality in Sentence Embeddings},
    author = {Dasgupta, Ishita and Guo, Demi and Stuhlm{\"u}ller, Andreas and Gershman, Samuel J and Goodman, Noah D},
    journal = {arXiv preprint 1802.04302},
    year = {2018}
}

@inproceedings{adi2016fine,
    title = {Fine-grained analysis of sentence embeddings using auxiliary prediction tasks},
    author = {Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer and Goldberg, Yoav},
    booktitle = {International Conference on Learning Representations},
    year = {2017},
    url = {https://openreview.net/forum?id=BJh6Ztuxl}
}

@inproceedings{kuncoro2018lstms,
    title = {{LSTM}s Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better},
    author = {Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-1132},
    pages = {1426--1436},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-1132},
    month = {July}
}

@inproceedings{poliak2018collecting,
    title = {Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation},
    author = {Poliak, Adam and Haldar, Aparajita and Rudinger, Rachel and Hu, J. Edward and Pavlick, Ellie and White, Aaron Steven and Van Durme, Benjamin},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1007},
    pages = {67--81},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1007},
    month = {October-November}
}

@article{ahmad2018multi,
    title = {Multi-task Learning for Universal Sentence Representations: What Syntactic and Semantic Information is Captured?},
    author = {Ahmad, Wasi Uddin and Bai, Xueying and Huang, Zhechao and Jiang, Chao and Peng, Nanyun and Chang, Kai-Wei},
    journal = {arXiv preprint 1804.07911},
    year = {2018}
}

@inproceedings{shi2016does,
    title = {Does String-Based Neural {MT} Learn Source Syntax?},
    author = {Shi, Xing and Padhi, Inkit and Knight, Kevin},
    booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    year = {2016},
    doi = {10.18653/v1/D16-1159},
    pages = {1526--1534},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D16-1159},
    month = {November}
}

@inproceedings{belinkov2017neural,
    title = {What do Neural Machine Translation Models Learn about Morphology?},
    author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
    booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2017},
    doi = {10.18653/v1/P17-1080},
    pages = {861--872},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P17-1080},
    month = {July}
}

@inproceedings{belinkov2017evaluating,
    title = {Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks},
    author = {Belinkov, Yonatan and M{\`a}rquez, Llu{\'\i}s and Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Glass, James},
    booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    year = {2017},
    pages = {1--10},
    publisher = {Asian Federation of Natural Language Processing},
    url = {https://www.aclweb.org/anthology/I17-1001},
    month = {November}
}

@phdthesis{belinkov2018thesis,
    title = {On internal language representations in deep learning: An analysis of machine translation and speech recognition},
    author = {Belinkov, Yonatan},
    year = {2018},
    school = {Massachusetts Institute of Technology}
}

@article{linzen2016assessing,
    title = {Assessing the Ability of {LSTM}s to Learn Syntax-Sensitive Dependencies},
    author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {4},
    number = {1},
    pages = {521--535},
    year = {2016},
    doi = {10.1162/tacl_a_00115},
    url = {https://www.aclweb.org/anthology/Q16-1037}
}

@inproceedings{gulordava2018colorless,
    title = {Colorless Green Recurrent Networks Dream Hierarchically},
    author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-1108},
    pages = {1195--1205},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-1108},
    month = {June}
}

@inproceedings{tran2018importance,
    title = {The Importance of Being Recurrent for Modeling Hierarchical Structure},
    author = {Tran, Ke and Bisazza, Arianna and Monz, Christof},
    year = {2018},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    doi = {10.18653/v1/D18-1503},
    pages = {4731--4736},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1503},
    month = {October-November}
}

@inproceedings{ettinger2018assessing,
    author = {Ettinger, Allyson and Elgohary, Ahmed and Phillips, Colin and Resnik, Philip},
    title = {Assessing Composition in Sentence Vector Representations},
    booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
    year = {2018},
    pages = {1790--1801},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/C18-1152},
    month = {August}
}

@inproceedings{gururangan2018annotation,
    title = {Annotation Artifacts in Natural Language Inference Data},
    author = {Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    volume = {2},
    pages = {107--112},
    year = {2018},
    doi = {10.18653/v1/N18-2017},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-2017},
    month = {June}
}

@inproceedings{poliak2018hypothesis,
    title = {Hypothesis Only Baselines in Natural Language Inference},
    author = {Poliak, Adam and Naradowsky, Jason and Haldar, Aparajita and Rudinger, Rachel and Van Durme, Benjamin},
    booktitle = {Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics},
    year = {2018},
    doi = {10.18653/v1/S18-2023},
    pages = {180--191},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/S18-2023},
    month = {June}
}

@article{punyakanok2008importance,
    title = {The Importance of Syntactic Parsing and Inference in Semantic Role Labeling},
    author = {Punyakanok, Vasin and Roth, Dan and Yih, Wen-tau},
    journal = {Computational Linguistics},
    volume = {34},
    number = {2},
    pages = {257--287},
    year = {2008},
    doi = {10.1162/coli.2008.34.2.257},
    url = {https://www.aclweb.org/anthology/J08-2005}
}

@inproceedings{gildea2002necessity,
    title = {The Necessity of Parsing for Predicate Argument Recognition},
    author = {Gildea, Daniel and Palmer, Martha},
    booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
    year = {2002},
    doi = {10.3115/1073083.1073124},
    pages = {239--246},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P02-1031},
    month = {July}
}

@inproceedings{hewitt-manning-2019-structural,
    title = {{A} Structural Probe for Finding Syntax in Word Representations},
    author = {Hewitt, John and Manning, Christopher D.},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1419},
    doi = {10.18653/v1/N19-1419},
    pages = {4129--4138},
    abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network{'}s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models{'} vector geometry.}
}

@inproceedings{hewitt-liang-2019-designing,
    title = {Designing and Interpreting Probes with Control Tasks},
    author = {Hewitt, John and Liang, Percy},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    month = {nov},
    year = {2019},
    address = {Hong Kong, China},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D19-1275},
    doi = {10.18653/v1/D19-1275},
    pages = {2733--2743},
    abstract = {Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe{'}s capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.}
}

@inproceedings{lee2017end,
    title = {End-to-end Neural Coreference Resolution},
    author = {Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
    year = {2017},
    doi = {10.18653/v1/D17-1018},
    pages = {188--197},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D17-1018},
    month = {September}
}

@inproceedings{lee2018higher,
    title = {Higher-Order Coreference Resolution with Coarse-to-Fine Inference},
    author = {Lee, Kenton and He, Luheng and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    year = {2018},
    doi = {10.18653/v1/N18-2108},
    pages = {687--692},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-2108},
    month = {June}
}

@article{dozat2016deep,
    title = {Deep biaffine attention for neural dependency parsing},
    author = {Dozat, Timothy and Manning, Christopher D},
    journal = {arXiv preprint 1611.01734},
    year = {2016}
}

@inproceedings{he2018jointly,
    author = {He, Luheng and Lee, Kenton and Levy, Omer and Zettlemoyer, Luke},
    title = {Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    year = {2018},
    doi = {10.18653/v1/P18-2058},
    pages = {364--369},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P18-2058},
    month = {July}
}

@article{hochreiter1997long,
    title = {Long short-term memory},
    author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
    journal = {Neural computation},
    volume = {9},
    number = {8},
    pages = {1735--1780},
    year = {1997}
}

@inproceedings{vaswani2017attention,
    title = {Attention is all you need},
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    booktitle = {Proceedings of NIPS},
    year = {2017}
}

@inproceedings{strubell2018semantics,
    author = {Strubell, Emma and McCallum, Andrew},
    title = {Syntax Helps {ELM}o Understand Semantics: Is Syntax Still Relevant in a Deep Neural Architecture for {SRL}?},
    booktitle = {Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for {NLP}},
    year = {2018},
    doi = {10.18653/v1/W18-2904},
    pages = {19--27},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-2904},
    month = {July}
}

@inproceedings{strubell2018linguistically,
    title = {Linguistically-Informed Self-Attention for Semantic Role Labeling},
    author = {Strubell, Emma and Verga, Patrick and Andor, Daniel and Weiss, David and McCallum, Andrew},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018},
    doi = {10.18653/v1/D18-1548},
    pages = {5027--5038},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D18-1548},
    month = {October-November}
}

@inproceedings{sennrich2016neural,
    title = {Neural Machine Translation of Rare Words with Subword Units},
    author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    year = {2016},
    doi = {10.18653/v1/P16-1162},
    pages = {1715--1725},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P16-1162},
    month = {August}
}

@article{wu16gnmt,
    title = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
    author = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
    year = {2016},
    URL = {http://arxiv.org/abs/1609.08144},
    journal = {CoRR},
    volume = {abs/1609.08144}
}

@inproceedings{sutskever2014sequence,
    title = {Sequence to sequence learning with neural networks},
    author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
    booktitle = {Proceedings of NIPS},
    year = {2014}
}

@inproceedings{evaluating-fine-grained-semantic-phenomena-in-neural-machine-translation-encoders-using-entailment,
    author = {Poliak, Adam and Belinkov, Yonatan and Glass, James and Van Durme, Benjamin},
    title = {On the Evaluation of Semantic Phenomena in Neural Machine Translation Using Natural Language Inference},
    year = {2018},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
    doi = {10.18653/v1/N18-2082},
    pages = {513--523},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-2082},
    month = {June}
}

@proceedings{SemEval:2010,
    editor = {Erk, Katrin and Strapparava, Carlo},
    title = {Proceedings of the 5th International Workshop on Semantic Evaluation},
    month = {July},
    year = {2010},
    address = {Uppsala, Sweden},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/S10-1000}
}

@inproceedings{hendrickx2009semeval,
    title = {{S}em{E}val-2010 Task 8: Multi-Way Classification of Semantic Relations Between Pairs of Nominals},
    author = {Hendrickx, Iris and Kim, Su Nam and Kozareva, Zornitsa and Nakov, Preslav and {\'O} S{\'e}aghdha, Diarmuid and Pad{\'o}, Sebastian and Pennacchiotti, Marco and Romano, Lorenza and Szpakowicz, Stan},
    booktitle = {Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009)},
    year = {2009},
    pages = {94--99},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W09-2415},
    month = {June}
}

@article{amigo2009comparison,
    title = {A comparison of extrinsic clustering evaluation metrics based on formal constraints},
    author = {Amig{\'o}, Enrique and Gonzalo, Julio and Artiles, Javier and Verdejo, Felisa},
    journal = {Information retrieval},
    volume = {12},
    number = {4},
    pages = {461--486},
    year = {2009},
    publisher = {Springer}
}

@inproceedings{bagga-baldwin-1998-bcubed,
    title = "Entity-Based Cross-Document Coreferencing Using the Vector Space Model",
    author = "Bagga, Amit  and
      Baldwin, Breck",
    booktitle = "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",
    month = aug,
    year = "1998",
    address = "Montreal, Quebec, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P98-1012",
    doi = "10.3115/980845.980859",
    pages = "79--85",
}


@inproceedings{bastings-etal-2019-interpretable,
    title = {Interpretable Neural Predictions with Differentiable Binary Variables},
    author = {Bastings, Jasmijn and Aziz, Wilker and Titov, Ivan},
    booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2019},
    address = {Florence, Italy},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P19-1284},
    doi = {10.18653/v1/P19-1284},
    pages = {2963--2977},
    abstract = {The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification{--}a rationale{--}for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.}
}

@article{church-2007-pendulum,
    title = "A Pendulum Swung Too Far",
    author = "Church, Kenneth",
    journal = "Linguistic Issues in Language Technology",
    volume = "2",
    year = "2007"
}

@book{chomsky-1957-syntactic,
    title = "Syntactic Structures",
    author = "Chomsky, Noam",
    year = "1957",
    address = "The Hague",
    publisher = "Mouton \& Co."
}

@book{chomsky-1965-aspects,
    title = "Aspects of the Theory of Syntax",
    author = "Chomsky, Noam",
    year = "1965",
    address = "Cambridge, MA",
    publisher = "MIT Press"
}

@inproceedings{krizhevsky-etal-2012-imagenet,
    author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {ImageNet Classification with Deep Convolutional Neural Networks},
    url = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
    volume = {25},
    year = {2012}
}

@inproceedings{levy-goldberg-2014-neural,
    author = {Levy, Omer and Goldberg, Yoav},
    title = {Neural Word Embedding as Implicit Matrix Factorization},
    year = {2014},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
    pages = {2177--2185},
    numpages = {9},
    location = {Montreal, Canada},
    series = {NIPS'14}
}

@inproceedings{mccann-etal-2017-learned,
  title={Learned in Translation: Contextualized Word Vectors},
  author={Bryan McCann and James Bradbury and Caiming Xiong and Richard Socher},
  booktitle={NIPS},
  year={2017}
}

@misc{sutton-2019-bitter,
    title = {The Bitter Lesson},
    author = {Rich Sutton},
    year = {2019},
    url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html}
}

@misc{norvig-2011-chomsky,
    title = {On Chomsky and the Two Cultures of Statistical Learning},
    author = {Peter Norvig},
    year = {2011},
    url = {https://norvig.com/chomsky.html}
}

@article{hooker-2020-hardware,
    author = {{Hooker}, Sara},
    title = "{The Hardware Lottery}",
    year = 2020,
    url = {https://arxiv.org/abs/1911.05248}
}   

@article{breiman-2001-statistical,
  title={Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)},
  author={Leo Breiman},
  journal={Statistical Science},
  year={2001},
  volume={16},
  pages={199--231}
}

@article{marcus-davis-2020-gpt3,
  title={GPT-3, Bloviator: OpenAI's language generator has no idea what it's talking about},
  author={Gary Marcus and Ernest Davis},
  journal={MIT Technology Review},
  year={2020}
  url={https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/}
}

@article{hofstadter-2018-shallowness,
  title={The Shallowness of Google Translate},
  author={Douglad Hofstadter},
  journal={The Atlantic},
  year={2018}
  url={https://www.theatlantic.com/technology/archive/2018/01/the-shallowness-of-google-translate/551570/}
}

@article{gold-1967-language,
  title={Language Identification in the Limit},
  author={E Mark Gold},
  journal={Information and Control},
  year={1967},
  volume={10},
  pages={447--474}
}

@phdthesis{horning-1969-study,
  title={A Study of Grammatical Inference},
  author={James Jay Horning},
  school={Stanford University},
  address={Stanford, CA},
  year={1969},
  month={aug}
}

@article{box-1976-science,
  author = {George E. P. Box},
  title = {Science and Statistics},
  journal = {Journal of the American Statistical Association},
  volume = {71},
  number = {356},
  pages = {791-799},
  year  = {1976},
  publisher = {Taylor & Francis},
  doi = {10.1080/01621459.1976.10480949},
  URL = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1976.10480949}
}

@article{lari-young-1990-applications,
  title={Applications of stochastic context-free grammars using the Inside-Outside algorithm},
  author={K. Lari and S. Young},
  journal={Computer Speech \& Language},
  year={1990},
  volume={5},
  pages={237-257}
}

@misc{levshina-hawkins-2021-loose,
  author       = {Natalia Levshina and
                  John A. Hawkins},
  title        = {{Loose and tight languages:  A typology based on 
                   associations between constructions and lexemes}},
  month        = aug,
  year         = 2021,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.5215108},
  url          = {https://doi.org/10.5281/zenodo.5215108}
}

@inbook{bender-emerson-toappear-computational,
  author    = "Emily M. Bender and Guy Emerson",
  editor    = "Stefan M\"{u}ller and Anne Abeill\'{e} and Robert D. Borsley and Jean-Pierre Koenig",
  title     = "Computational Linguistics and Grammar Engineering",
  booktitle = "Head-Driven Phrase Structure Grammar: The Handbook",
  publisher = "Language Science Press",
  address   = "Berlin",
  note = "To appear."
  year = "2021"
  @Comment pages     = "187--221"
}

@article{oepen-etal-2004-lingo,
  title  = {{L}in{GO} {R}edwoods: A Rich and Dynamic Treebank for {HPSG}},
  author = {Stephan Oepen and Dan Flickinger and Kristina Toutanova and Christopher D. Manning},
  journal = {Research on Language and Computation},
  year = {2004},
  volume = {2},
  pages = {575--596},
  doi = {10.1007/s11168-004-7430-4}
}

@inbook{flickinger-etal-2017-sustainable,
  author="Flickinger, Dan and Oepen, Stephan and Bender, Emily M.",
  editor="Ide, Nancy and Pustejovsky, James",
  title="Sustainable Development and Refinement of Complex Linguistic Annotations at Scale",
  booktitle="Handbook of Linguistic Annotation",
  year="2017",
  publisher="Springer Netherlands",
  address="Dordrecht",
  pages="353--377",
  abstract="The development of complex and consistent linguistic annotations over large and varied corpora requires an approach which allows for the incremental improvement of existing annotations by encoding all manual effort in such a way that its value is preserved and enhanced even as the resource is improved over time. This manual effort includes both annotation design and disambiguation; in the case of syntactico-semantic annotations, the former can be encoded in a machine-readable grammar and the latter as a series of decisions made at a level of granularity which supports both efficient human disambiguation and later machine re-use of the individual decisions. The general approach can be applied beyond syntactico-semantic annotation to any annotation project where the design of the representations can be encoded as a grammar, and thus we frame our methodological discussion in terms of incremental improvement, with syntactico-semantic annotations as a case study.",
  isbn="978-94-024-0881-2",
  doi="10.1007/978-94-024-0881-2_14",
  url="https://doi.org/10.1007/978-94-024-0881-2_14"
}

@article{dowty-1991-thematic,
 ISSN = {00978507, 15350665},
 URL = {http://www.jstor.org/stable/415037},
 abstract = {As a novel attack on the perennially vexing questions of the theoretical status of thematic roles and the inventory of possible roles, this paper defends a strategy of basing accounts of roles on more unified domains of linguistic data than have been used in the past to motivate roles, addressing in particular the problem of ARGUMENT SELECTION (principles determining which roles are associated with which grammatical relations). It is concluded that the best theory for describing this domain is not a traditional system of discrete roles (Agent, Patient, Source, etc.) but a theory in which the only roles are two cluster-concepts called Proto-Agent and Proto-Patient, each characterized by a set of verbal entailments: an argument of a verb may bear either of the two proto-roles (or both) to varying degrees, according to the number of entailments of each kind the verb gives it. Both fine-grained and coarse-grained classes of verbal arguments (corresponding to traditional thematic roles and other classes as well) follow automatically, as do desired 'role hierarchies'. By examining occurrences of the 'same' verb with different argument configurations-e.g. two forms of psych predicates and object-oblique alternations as in the familiar spray/load class-it can also be argued that proto-roles act as defaults in the learning of lexical meanings. Are proto-role categories manifested else-where in language or as cognitive categories? If so, they might be a means of making grammar acquisition easier for the child, they might explain certain other typological and acquisitional observations, and they may lead to an account of contrasts between unaccusative and unergative intransitive verbs that does not rely on deriving unaccusatives from underlying direct objects.},
 author = {David Dowty},
 journal = {Language},
 number = {3},
 pages = {547--619},
 publisher = {Linguistic Society of America},
 title = {Thematic Proto-Roles and Argument Selection},
 volume = {67},
 year = {1991}
}

@inproceedings{seo-etal-2017-bidirectional,
  author    = {Min Joon Seo and
               Aniruddha Kembhavi and
               Ali Farhadi and
               Hannaneh Hajishirzi},
  title     = {Bidirectional Attention Flow for Machine Comprehension},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=HJ0UKP9ge},
  timestamp = {Thu, 25 Jul 2019 14:25:45 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/SeoKFH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{tenney-etal-2019-what,
  author    = {Ian Tenney and
               Patrick Xia and
               Berlin Chen and
               Alex Wang and
               Adam Poliak and
               R. Thomas McCoy and
               Najoung Kim and
               Benjamin Van Durme and
               Samuel R. Bowman and
               Dipanjan Das and
               Ellie Pavlick},
  title     = {What do you learn from context? Probing for sentence structure in
               contextualized word representations},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=SJzSgnRcKX},
  timestamp = {Thu, 25 Jul 2019 14:25:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/TenneyXCWPMKDBD19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{miller-etal-1990-introduction,
  title={Introduction to WordNet: An On-line Lexical Database},
  author={G. Miller and R. Beckwith and C. Fellbaum and Derek Gross and K. Miller},
  journal={International Journal of Lexicography},
  year={1990},
  volume={3},
  pages={235-244}
}

@book{gordon-hobbs-2017-formal,
  title={A Formal Theory of Commonsense Psychology: How People Think People Think},
  author={Andrew S. Gordon and Jerry R. Hobbs},
  publisher={Cambridge University Press},
  address={Cambridge, UK},
  year={2017}
}

@article{lenat-1995-cyc,
  author    = {Douglas B. Lenat},
  title     = {{CYC:} {A} Large-Scale Investment in Knowledge Infrastructure},
  journal   = {Commun. {ACM}},
  volume    = {38},
  number    = {11},
  pages     = {32--38},
  year      = {1995},
  url       = {https://doi.org/10.1145/219717.219745},
  doi       = {10.1145/219717.219745},
  timestamp = {Sun, 02 Jun 2019 20:48:57 +0200},
  biburl    = {https://dblp.org/rec/journals/cacm/Lenat95.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{speer-etal-2017-conceptnet,
  author    = {Robyn Speer and
               Joshua Chin and
               Catherine Havasi},
  editor    = {Satinder P. Singh and
               Shaul Markovitch},
  title     = {ConceptNet 5.5: An Open Multilingual Graph of General Knowledge},
  booktitle = {Proceedings of the Thirty-First {AAAI} Conference on Artificial Intelligence,
               February 4-9, 2017, San Francisco, California, {USA}},
  pages     = {4444--4451},
  publisher = {{AAAI} Press},
  year      = {2017},
  url       = {http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972},
  timestamp = {Wed, 10 Feb 2021 08:45:07 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/SpeerCH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{minsky-papert-1969-perceptrons,
    title = {Perceptrons: an Introduction to Computational Geometry},
    author = {Marvin Minsky and Seymour Papert},
    isbn = {0262130432},
    year = {1969},
    publisher = {MIT Press}
    address = {Cambridge, MA}
}

@book{bacon-1620-novum,
  title = {Novum Organum},
  author = {Francis Bacon},
  year = {1620},
  address = {England}
}

@article{baroni-2021-proper,
  title={On the proper role of linguistically-oriented deep net analysis in linguistic theorizing},
  author={Marco Baroni},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.08694}
}

@book{jurafsky-martin-2008,
  title = {Speech and Language Processing},
  author = {Dan Jurafsky and James Martin},
  year = {2008},
  publisher = {Prentice Hall},
  address = {Upper Saddle River, NJ},
  edition = {2nd}
}

@inproceedings{kharitonov-chaabouni-2021-what,
  author    = {Eugene Kharitonov and
               Rahma Chaabouni},
  title     = {What they do when in doubt: a study of inductive biases in seq2seq
               learners},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=YmA86Zo-P\_t},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KharitonovC21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pustejovsky-2011-coercion,
author = {James Pustejovsky},
doi = {doi:10.1515/ling.2011.039},
url = {https://doi.org/10.1515/ling.2011.039},
title = {Coercion in a general theory of argument selection},
journal = {Linguistics},
number = {6},
volume = {49},
year = {2011},
pages = {1401--1431}
}

@book{cinque-1999-adverbs,
title={Adverbs and Functional Heads: A Crosslinguistic Perspective},
author={Guglielmo Cinque},
year={1999},
publisher={Oxford University Press},
address={Oxford, UK}
}

@inbook{haspelmath-2010-framework,
  author       = {Haspelmath, Martin},
  title        = {Framework-Free Grammatical Theory},
  month        = jan,
  year         = 2010,
  editor       = "Bernd Heine and Heiko Narrog",
  booktitle    = "The Oxford Handbook of Linguistic Analysis",
  publisher    = {Oxford University Press},
  address      = "Oxford, UK",
}

@article{stanojevic-steedman-2021-formal,
    author = {Stanojevi{\'c}, Milo{\v{s}} and Steedman, Mark},
    title = "{Formal Basis of a Language Universal}",
    journal = {Computational Linguistics},
    volume = {47},
    number = {1},
    pages = {9--42},
    year = {2021},
    month = {04},
    issn = {0891--2017},
    doi = {10.1162/coli_a_00394},
    url = {https://doi.org/10.1162/coli\_a\_00394},
    eprint = {https://direct.mit.edu/coli/article-pdf/47/1/9/1911502/coli\_a\_00394.pdf}
}

@misc{davidson-1967-logical,
  title={The Logical Form of Action Sentences},
  author={Donald Davidson},
  year={1967}
}

@ARTICLE{shannon-1948-mathematical,
  author={Shannon, Claude E.},
  journal={The Bell System Technical Journal}, 
  title={A mathematical theory of communication}, 
  year={1948},
  volume={27},
  number={3},
  pages={379-423},
  doi={10.1002/j.1538-7305.1948.tb01338.x}
  }

@inproceedings{collobert-weston-2008-deep,
  author = {Collobert, Ronan and Weston, Jason},
  title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
  year = {2008},
  isbn = {9781605582054},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/1390156.1390177},
  doi = {10.1145/1390156.1390177},
  booktitle = {Proceedings of the 25th International Conference on Machine Learning},
  pages = {160--167},
  numpages = {8},
  location = {Helsinki, Finland},
  series = {ICML '08}
}

@inproceedings{brown-etal-2020-language,
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title = {Language Models are Few-Shot Learners},
  url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume = {33},
  year = {2020}
}

@article{radford-etal-2019-language,
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title = {{Language Models are Unsupervised Multitask Learners}},
  url = {https://openai.com/blog/better-language-models/},
  year = {2019}
}

@misc{michael-2020-dissect,
  author = {Julian Michael},
  title = {To Dissect an Octopus: Making Sense of the Form/Meaning Debate},
  year = {2020},
  month = {jul},
  url = {https://blog.julianmichael.org/2020/07/23/to-dissect-an-octopus.html}
}

@misc{damour-etal-2020-underspecification,
      title={Underspecification Presents Challenges for Credibility in Modern Machine Learning}, 
      author={Alexander D'Amour and Katherine Heller and Dan Moldovan and Ben Adlam and Babak Alipanahi and Alex Beutel and Christina Chen and Jonathan Deaton and Jacob Eisenstein and Matthew D. Hoffman and Farhad Hormozdiari and Neil Houlsby and Shaobo Hou and Ghassen Jerfel and Alan Karthikesalingam and Mario Lucic and Yian Ma and Cory McLean and Diana Mincu and Akinori Mitani and Andrea Montanari and Zachary Nado and Vivek Natarajan and Christopher Nielson and Thomas F. Osborne and Rajiv Raman and Kim Ramasamy and Rory Sayres and Jessica Schrouff and Martin Seneviratne and Shannon Sequeira and Harini Suresh and Victor Veitch and Max Vladymyrov and Xuezhi Wang and Kellie Webster and Steve Yadlowsky and Taedong Yun and Xiaohua Zhai and D. Sculley},
      year={2020},
      eprint={2011.03395},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{weld-bansal-2019-challenge,
  author = {Weld, Daniel S. and Bansal, Gagan},
  title = {The Challenge of Crafting Intelligible Intelligence},
  year = {2019},
  issue_date = {June 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {62},
  number = {6},
  issn = {000--0782},
  url = {https://doi.org/10.1145/3282486},
  doi = {10.1145/3282486},
  journal = {Commun. ACM},
  month = may,
  pages = {70--79},
  numpages = {10}
}

@book{weizenbaum-1976-computer,
  author = {Weizenbaum, Joseph},
  title = {Computer Power and Human Reason: From Judgment to Calculation},
  year = {1976},
  isbn = {0716704641},
  publisher = {W. H. Freeman \& Co.},
  address = {USA},
}

@article{olah-etal-2020-zoom,
  title={Zoom In: An Introduction to Circuits},
  author={Christopher Olah and Nick Cammarata and L. Schubert and Gabriel Goh and Michael Petrov and Shan Carter},
  year={2020},
  journal={Distill},
  volume={5},
  number={3},
  doi={10.23915/distill.00024.001}
}

@book{james-1907-pragmatism,
  title={Pragmatism: a New Name for some Old Ways of Thinking},
  author={William James},
  year={1907}
}

@article{palmer-etal-2006-making,
  title={Making fine-grained and coarse-grained sense distinctions, both manually and automatically},
  author={Martha Palmer and H. Dang and C. Fellbaum},
  journal={Natural Language Engineering},
  year={2006},
  volume={13},
  pages={137--163}
}

@article{white-rawlins-2016-computational,
  title={A Computational Model of S-Selection},
  author={Aaron Steven White and Kyle Rawlins},
  journal={Semantics and Linguistic Theory},
  editor={Mary Moroney, Carol-Rose Little, Jacob Collard, and Dan Burgdorf},
  volume={26},
  pages={641--663},
  doi={10.3765/salt.v26i0.3819}
}

@inproceedings{white-rawlins-2018-role,
  title={The Role of Veridicality and Factivity in Clause Selection},
  author={Aaron Steven White and Kyle Rawlins},
  year={2018},
  booktitle={Proceedings of the 48th Annual Meeting of the North East Linguistic Society},
  editor={Sherry Hucklebridge and Max Nelson},
  pages={221--234},
  publisher={GLSA Publications},
  address={Amherst, MA}
}

@article{white-accepted-believing,
title={On Believing and Hoping Whether},
author={Aaron Steven White},
journal={Semantics \& Pragmatics},
note={Accepted with revisions.}
}

@incollection{dijkstra-1974-role,
  title={EWD 447: On the role of scientific thought},
  author={Edsger W. Dijkstra},
  booktitle={Selected Writings on Computing: A Personal Perspective},
  year={1974},
  publisher={Springer-Verlag},
  pages={60--66},
  isbn={0387906525},
  note={Book published in 1982.}
}

@book{pierce-etal-2017-software,
  author = {Benjamin C. Pierce and Arthur Azevedo de Amorim
                  and Chris Casinghino and Marco Gaboardi and
                  Michael Greenberg and C\v{a}t\v{a}lin Hri\c{t}cu
                  and Vilhelm Sj\"{o}berg and Brent Yorgey},
  title = {Software Foundations},
  year = {2017},
  publisher = {Electronic textbook},
  plclub = {Yes},
  bcp = {Yes},
  keys = {verification,books},
  note = {Version 5.0. http://www.cis.upenn.edu/~bcpierce/sf},
  ebook = {http://www.cis.upenn.edu/~bcpierce/sf},
  japanese = {http://proofcafe.org/sf}
}

@inproceedings{lake-baroni-2018-generalization,
  author    = {Brenden M. Lake and
               Marco Baroni},
  editor    = {Jennifer G. Dy and
               Andreas Krause},
  title     = {Generalization without Systematicity: On the Compositional Skills
               of Sequence-to-Sequence Recurrent Networks},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {2879--2888},
  publisher = {{PMLR}},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v80/lake18a.html},
  timestamp = {Wed, 03 Apr 2019 18:17:30 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/LakeB18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{turing-1950-computing,
  title={Computing Machinery and Intelligence},
  author={Alan M. Turing},
  journal={Mind},
  volume={LIX},
  number={236},
  month={oct},
  year={1950},
  pages={433--460},
  doi={10.1093/mind/LIX.236.433}
}

@InProceedings{dagan-etal-2006-pascal,
  author="Dagan, Ido
  and Glickman, Oren
  and Magnini, Bernardo",
  editor="Qui{\~{n}}onero-Candela, Joaquin
  and Dagan, Ido
  and Magnini, Bernardo
  and d'Alch{\'e}-Buc, Florence",
  title="The PASCAL Recognising Textual Entailment Challenge",
  booktitle="Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment",
  year="2006",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="177--190",
  abstract="This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.",
  isbn="978-3-540-33428-6"
}

@article{demszky-etal-2018-transforming,
  author    = {Dorottya Demszky and
               Kelvin Guu and
               Percy Liang},
  title     = {Transforming Question Answering Datasets Into Natural Language Inference
               Datasets},
  journal   = {CoRR},
  volume    = {abs/1809.02922},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.02922},
  eprinttype = {arXiv},
  eprint    = {1809.02922},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-02922.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gardner-etal-2019-question,
  author    = {Matt Gardner and
               Jonathan Berant and
               Hannaneh Hajishirzi and
               Alon Talmor and
               Sewon Min},
  title     = {Question Answering is a Format; When is it Useful?},
  journal   = {CoRR},
  volume    = {abs/1909.11291},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.11291},
  eprinttype = {arXiv},
  eprint    = {1909.11291},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-11291.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mccarthy-1976-example,
  title={An Example for Natural Language Understanding and the AI Problems it Raises},
  author={John McCarthy},
  year={1976},
  url={http://jmc.stanford.edu/articles/mrhug/mrhug.pdf}
}

@inproceedings{voorhees-1999-trec,
    author = {Ellen M. Voorhees},
    title = {The TREC-8 Question Answering Track Report},
    booktitle = {In Proceedings of TREC-8},
    year = {1999},
    pages = {77--82}
}

@article{mccann-etal-2018-natural,
  title={The Natural Language Decathlon: Multitask Learning as Question Answering},
  author={Bryan McCann and N. Keskar and Caiming Xiong and R. Socher},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.08730}
}

@misc{cooper-etal-1996-using,
    author = {Robin Cooper and Dick Crouch and Jan Van Eijck and Chris Fox and Josef Van Genabith and Jan Jaspars and Hans Kamp and David Milward and Manfred Pinkal and Massimo Poesio and Steve Pulman and Ted Briscoe and Holger Maier and Karsten Konrad},
    title = {Using the Framework},
    year = {1996},
    note = {The FRACAS Consortium}
}

@inproceedings{stepanov-etal-2021-qaalign,
  title={{QA-Align}: Modeling Cross-Text Content Overlap through Question-Answer Propositions},
  author={Daniela Stepanov and Paul Roit and Ayal Klein and Ori Ernst and Ido Dagan},
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  address = "Punta Cana, Dominican Republic",
  publisher = "Association for Computational Linguistics",
  @Comment url = "https://aclanthology.org/D18-1007",
  @Comment doi = "10.18653/v1/D18-1007",
  @Comment pages = "67--81",
  note = "To appear."
}

@article{bingham-etal-2019-pyro,
  author    = {Eli Bingham and
               Jonathan P. Chen and
               Martin Jankowiak and
               Fritz Obermeyer and
               Neeraj Pradhan and
               Theofanis Karaletsos and
               Rohit Singh and
               Paul A. Szerlip and
               Paul Horsfall and
               Noah D. Goodman},
  title     = {Pyro: Deep Universal Probabilistic Programming},
  journal   = {J. Mach. Learn. Res.},
  volume    = {20},
  pages     = {28:1--28:6},
  year      = {2019},
  url       = {http://jmlr.org/papers/v20/18-403.html}
}

@inproceedings{koh-etal-2020-concept,
  author    = {Pang Wei Koh and
               Thao Nguyen and
               Yew Siang Tang and
               Stephen Mussmann and
               Emma Pierson and
               Been Kim and
               Percy Liang},
  title     = {Concept Bottleneck Models},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {5338--5348},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/koh20a.html},
  timestamp = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/KohNTMPKL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dozat-manning-2017-deep,
  author    = {Timothy Dozat and
               Christopher D. Manning},
  title     = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Hk95PK9le},
  timestamp = {Thu, 25 Jul 2019 14:25:56 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ilyas-etal-2019-adversarial,
 author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adversarial Examples Are Not Bugs, They Are Features},
 url = {https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{arjovsky-etal-2020-invariant,
      title={Invariant Risk Minimization}, 
      author={Martin Arjovsky and L\'{e}on Bottou and Ishaan Gulrajani and David Lopez-Paz},
      year={2020},
      eprint={1907.02893},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{peters-etal-2016-causal,
author = {Peters, Jonas and B\"{u}hlmann, Peter and Meinshausen, Nicolai},
title = {Causal inference by using invariant prediction: identification and confidence intervals},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {78},
number = {5},
pages = {947-1012},
keywords = {Causal discovery, Causal inference, Confidence intervals, Invariant prediction},
doi = {https://doi.org/10.1111/rssb.12167},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12167},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12167},
year = {2016}
}

